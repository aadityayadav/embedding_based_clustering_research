{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 23:14:43.015065: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import clustbench\n",
    "import genieclust\n",
    "from sklearn.cluster import (\n",
    "    KMeans, AgglomerativeClustering, DBSCAN, MeanShift, SpectralClustering,\n",
    "    AffinityPropagation, OPTICS, Birch, MiniBatchKMeans, SpectralCoclustering\n",
    ")\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.decomposition import (\n",
    "    PCA, KernelPCA, TruncatedSVD, FastICA, FactorAnalysis, LatentDirichletAllocation, NMF\n",
    ")\n",
    "from sklearn.manifold import (\n",
    "    TSNE, MDS, Isomap, LocallyLinearEmbedding, SpectralEmbedding\n",
    ")\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, rand_score, adjusted_rand_score,\n",
    "    fowlkes_mallows_score, mutual_info_score, adjusted_mutual_info_score, normalized_mutual_info_score\n",
    ")\n",
    "import hdbscan\n",
    "from kmodes.kmodes import KModes\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn import metrics\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel\n",
    ")\n",
    "import umap\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    \"\"\"Convert each row of X into a string, joined by spaces.\"\"\"\n",
    "    return [\" \".join(map(str, row)) for row in X]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Text-based Embedding Functions\n",
    "# -------------------------------------------------------------------\n",
    "def generate_TFIDF_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    vectorizer = TfidfVectorizer(max_features=500)\n",
    "    return vectorizer.fit_transform(X_as_str).toarray()\n",
    "\n",
    "def generate_CountVectorizer_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    vectorizer = CountVectorizer(max_features=500)\n",
    "    return vectorizer.fit_transform(X_as_str).toarray()\n",
    "\n",
    "def generate_CharTFIDF_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=500)\n",
    "    return vectorizer.fit_transform(X_as_str).toarray()\n",
    "\n",
    "def generate_Hashing_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    vectorizer = HashingVectorizer(n_features=500)\n",
    "    return vectorizer.transform(X_as_str).toarray()\n",
    "\n",
    "def generate_SentenceTransformer_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    st_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    return st_model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "def generate_DistilBERT_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def get_distilbert_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Mean pooling over token embeddings\n",
    "        return outputs.last_hidden_state.mean(dim=1).detach().numpy()[0]\n",
    "\n",
    "    return np.array([get_distilbert_embedding(txt) for txt in X_as_str])\n",
    "\n",
    "def generate_Doc2Vec_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    documents = [TaggedDocument(words=txt.split(), tags=[str(i)]) for i, txt in enumerate(X_as_str)]\n",
    "    doc2vec_model = Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
    "    doc2vec_model.build_vocab(documents)\n",
    "    doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "    return np.array([doc2vec_model.infer_vector(txt.split()) for txt in X_as_str])\n",
    "\n",
    "def generate_multilingual_e5_large_instruct_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    model = SentenceTransformer('intfloat/multilingual-e5-large-instruct')\n",
    "    return model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "def generate_KaLM_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    model = SentenceTransformer('HIT-TMG/KaLM-embedding-multilingual-mini-v1')\n",
    "    return model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "\n",
    "def generate_mxbai_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    model = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1')\n",
    "    return model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "def generate_bge_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    model = SentenceTransformer('BAAI/bge-reranker-large')\n",
    "    return model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Non-text Embedding Functions (unchanged)\n",
    "# -------------------------------------------------------------------\n",
    "# def generate_PCA_embedding(X):\n",
    "#     return PCA(n_components=2, whiten=False, random_state=42).fit_transform(X)\n",
    "\n",
    "# def generate_TSNE_embedding(X):\n",
    "#     return TSNE(\n",
    "#         n_components=2,\n",
    "#         perplexity=30,\n",
    "#         learning_rate='auto',\n",
    "#         max_iter=1000,\n",
    "#         random_state=42,\n",
    "#         init='pca'\n",
    "#     ).fit_transform(X)\n",
    "\n",
    "# def generate_UMAP_embedding(X):\n",
    "#     return umap.UMAP(\n",
    "#         n_components=2,\n",
    "#         n_neighbors=15,\n",
    "#         min_dist=0.1,\n",
    "#         metric='euclidean',\n",
    "#         random_state=42\n",
    "#     ).fit_transform(X)\n",
    "\n",
    "# def generate_MDS_embedding(X):\n",
    "#     return MDS(n_components=2, metric=True, random_state=42, n_init=4, max_iter=300).fit_transform(X)\n",
    "\n",
    "# def generate_Isomap_embedding(X):\n",
    "#     return Isomap(n_components=2, n_neighbors=5).fit_transform(X)\n",
    "\n",
    "# def generate_LLE_embedding(X):\n",
    "#     return LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42).fit_transform(X)\n",
    "\n",
    "# def generate_SpectralEmbedding_embedding(X):\n",
    "#     return SpectralEmbedding(\n",
    "#         n_components=2,\n",
    "#         n_neighbors=5,\n",
    "#         random_state=42\n",
    "#     ).fit_transform(X)\n",
    "\n",
    "# def generate_KernelPCA_embedding(X):\n",
    "#     return KernelPCA(\n",
    "#         n_components=2,\n",
    "#         kernel='rbf',\n",
    "#         gamma=None,\n",
    "#         random_state=42\n",
    "#     ).fit_transform(X)\n",
    "\n",
    "# def generate_Gaussian_random_projection(X):\n",
    "#     return GaussianRandomProjection(n_components=2, eps=0.1, random_state=42).fit_transform(X)\n",
    "\n",
    "# def generate_TruncatedSVD_embedding(X):\n",
    "#     return TruncatedSVD(n_components=2, random_state=42).fit_transform(X)\n",
    "\n",
    "# def generate_FastICA_embedding(X):\n",
    "#     return FastICA(n_components=2, whiten=True, max_iter=200, random_state=42).fit_transform(X)\n",
    "\n",
    "# def generate_FactorAnalysis_embedding(X):\n",
    "#     return FactorAnalysis(n_components=2, random_state=42).fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(X):\n",
    "    # Precompute the text representation once\n",
    "    X_as_str = preprocess_data(X)\n",
    "    \n",
    "    # List your embedding functions as tuples: (name, function, is_text_based)\n",
    "    embedding_functions = [\n",
    "        # (\"PCA\", generate_PCA_embedding, False),\n",
    "        # (\"KernelPCA\", generate_KernelPCA_embedding, False),\n",
    "        # (\"TruncatedSVD\", generate_TruncatedSVD_embedding, False),\n",
    "        # (\"FactorAnalysis\", generate_FactorAnalysis_embedding, False),\n",
    "        # (\"TSNE\", generate_TSNE_embedding, False),\n",
    "        # (\"UMAP\", generate_UMAP_embedding, False),\n",
    "        # (\"MDS\", generate_MDS_embedding, False),\n",
    "        # (\"Isomap\", generate_Isomap_embedding, False),\n",
    "        # (\"LLE\", generate_LLE_embedding, False),\n",
    "        # (\"SpectralEmbedding\", generate_SpectralEmbedding_embedding, False),\n",
    "        # (\"GaussianRP\", generate_Gaussian_random_projection, False),\n",
    "        (\"TFIDF\", generate_TFIDF_embedding, True),\n",
    "        (\"CountVectorizer\", generate_CountVectorizer_embedding, True),\n",
    "        (\"CharTFIDF\", generate_CharTFIDF_embedding, True),\n",
    "        (\"Hashing\", generate_Hashing_embedding, True),\n",
    "        (\"SentenceTransformer\", generate_SentenceTransformer_embedding, True),\n",
    "        (\"DistilBERT\", generate_DistilBERT_embedding, True),\n",
    "        (\"Doc2Vec\", generate_Doc2Vec_embedding, True),\n",
    "        (\"multilingual_e5_large_instruct\", generate_multilingual_e5_large_instruct_embedding, True),\n",
    "        (\"KaLM\", generate_KaLM_embedding, True),\n",
    "        (\"mxbai\", generate_mxbai_embedding, True),\n",
    "        (\"bge\", generate_bge_embedding, True),\n",
    "    ]\n",
    "    \n",
    "    results_dict = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {}\n",
    "        for name, func, is_text in embedding_functions:\n",
    "            # For text-based functions, pass the precomputed X_as_str; otherwise, just pass X.\n",
    "            if is_text:\n",
    "                futures[executor.submit(func, X, X_as_str)] = name\n",
    "            else:\n",
    "                futures[executor.submit(func, X)] = name\n",
    "\n",
    "        for future in futures:\n",
    "            func_name = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {func_name}: {e}\")\n",
    "                result = None\n",
    "            results_dict[func_name] = result\n",
    "\n",
    "    results_dict['Base'] = X\n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/gagolews/clustering-data-v1/raw/v1.1.0\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "def get_cached_embeddings(collection, dataset, X):\n",
    "    cache_dir = \"embedding_cache_2\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_file = os.path.join(cache_dir, f\"{collection}_{dataset}_embeddings.pkl\")\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(\"Loading embeddings from cache:\", cache_file)\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            embeddings = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Cache not found. Generating embeddings...\")\n",
    "        embeddings = generate_embeddings(X)\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        print(\"Embeddings cached at:\", cache_file)\n",
    "    return embeddings\n",
    "\n",
    "def load_data(collection, dataset):\n",
    "    benchmark = clustbench.load_dataset(collection, dataset, url=data_url)\n",
    "    X = benchmark.data\n",
    "    print(\"Loaded: \", X.shape[0], \" | Dimension: \", X.shape[1], \" | Label count: \", len(benchmark.labels))\n",
    "    print(\"Getting embeddings (with caching)...\")\n",
    "    X_embedded_dict = get_cached_embeddings(collection, dataset, X)\n",
    "    return X, benchmark, X_embedded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "each dataset can have multiple labels, \n",
    "pick one at a time and that defines your partition size, aka k\n",
    "\n",
    "Overall, Genie returned a clustering quite similar to the reference one. We may consider 107\n",
    "(namely, c11 + c22 + c33 ) out of the 120 input points as correctly grouped. In particular, \n",
    "all the red and green reference points (the 2nd and the 3rd row) have been properly discovered.\n",
    "\n",
    "Normalized Clustering Accuracy (NCA) \n",
    "NCA is the averaged percentage of correctly classified points in each cluster \n",
    "above the perfectly uniform label distribution.\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def predict(embedding_technique, X, label, benchmark, clustering_method, plot=False):\n",
    "\n",
    "    y_true = benchmark.labels[label] \n",
    "    (k := max(y_true))  # or benchmark.n_clusters[0]\n",
    "    m = max(min(y_true),2)\n",
    "    method = clustering_method.lower()\n",
    "\n",
    "    # Define the clustering model\n",
    "    if method == \"genie\":\n",
    "        model = genieclust.Genie(n_clusters=k)  # using default parameters\n",
    "    elif method == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    elif method == \"agglomerative\":\n",
    "        model = AgglomerativeClustering(n_clusters=k)\n",
    "    # elif method == \"dbscan\":\n",
    "    #     model = DBSCAN(eps=0.2, min_samples=m)\n",
    "    # elif method == \"meanshift\":\n",
    "    #     model = MeanShift()\n",
    "    elif method == \"spectral\":\n",
    "        model = SpectralClustering(n_clusters=k, random_state=42)\n",
    "    # elif method == \"affinitypropagation\":\n",
    "    #     model =  AffinityPropagation(random_state=42, convergence_iter= 5, max_iter= 100)\n",
    "    # elif method == \"optics\":\n",
    "    #     model = OPTICS()\n",
    "    elif method == \"gaussianmixture\":\n",
    "        model = GaussianMixture(n_components=k, random_state=42)\n",
    "    elif method == \"hdbscan\":\n",
    "        model = hdbscan.HDBSCAN(min_cluster_size=m)\n",
    "    elif method == \"kmodes\":\n",
    "        model = KModes(n_clusters=k, random_state=42, init=\"Huang\")\n",
    "    elif method == \"birch\":\n",
    "        model = Birch(n_clusters=k)\n",
    "    elif method == \"minibatchkmeans\":\n",
    "        model = MiniBatchKMeans(n_clusters=k, random_state=42)\n",
    "    elif method == \"kmedoids\":\n",
    "        model = KMedoids(n_clusters=k, random_state=42)\n",
    "    elif method == \"latentdirichletallocation\":\n",
    "        X = np.maximum(X, 0)\n",
    "        model = LatentDirichletAllocation(n_components=k, random_state=42)\n",
    "    elif method == \"spectralcoclustering\":\n",
    "        model =  SpectralCoclustering(n_clusters=k)\n",
    "    elif method == \"bayesiangaussianmixture\":\n",
    "        model = BayesianGaussianMixture(n_components=k)   \n",
    "\n",
    "    print(\"The model: \" +  method + \" has been trained now getting y_pred\") \n",
    "   \n",
    "    # Fit the model and predict the cluster labels\n",
    "    if method == \"gaussianmixture\":  # Gaussian uses predict instead of fit_predict\n",
    "        (y_pred := model.fit(X).predict(X) + 1)\n",
    "    elif method == \"latentdirichletallocation\":\n",
    "        model.fit(X)\n",
    "        y_pred = model.transform(X).argmax(axis=1) + 1\n",
    "    elif method == \"spectralcoclustering\":\n",
    "        model.fit(X)\n",
    "        y_pred = y_pred = model.row_labels_ + 1\n",
    "    # elif method == \"optics\" or method == \"hdbscan\" or method == \"dbscan\":\n",
    "    elif method == \"hdbscan\":\n",
    "        y_pred = model.fit_predict(X)\n",
    "        unique_labels = np.unique(y_pred)\n",
    "        if -1 in unique_labels:\n",
    "            y_pred = np.where(y_pred == -1, max(unique_labels) + 1, y_pred)  # Assign noise to a new cluster\n",
    "        y_pred += 1\n",
    "    else:\n",
    "        (y_pred := model.fit_predict(X) + 1)\n",
    "        \n",
    "    assert len(y_true) == len(y_pred), \"Length of y_true: \" + str(len(y_true)) +  \"and y_pred: \" + str(len(y_pred)) + \"are not the same\"\n",
    "    print(\"y_true dimensions: \" + str(y_true.shape))\n",
    "    print(\"y_pred dimensions: \" + str(y_pred.shape))\n",
    "    print(\"max(y_true): \" + str(max(y_true)))\n",
    "    print(\"max(y_pred): \" + str(max(y_pred)))\n",
    "    print(\"min(y_true): \" + str(min(y_true)))\n",
    "    print(\"min(y_pred): \" + str(min(y_pred)))\n",
    "\n",
    "    nca = clustbench.get_score(y_true, y_pred)\n",
    "    cf = metrics.confusion_matrix(y_true, y_pred)\n",
    "    r = rand_score(y_true, y_pred)\n",
    "    ar = adjusted_rand_score(y_true, y_pred)\n",
    "    fm = fowlkes_mallows_score(y_true, y_pred)\n",
    "    mi = mutual_info_score(y_true, y_pred)\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    ami = adjusted_mutual_info_score(y_true, y_pred)\n",
    "    a = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "\n",
    "\n",
    "    if plot:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        model.plots.plot_scatter(X, labels=y_true-1, axis=\"equal\", title=\"y_true\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        model.plots.plot_scatter(X, labels=y_pred-1, axis=\"equal\", title=\"y_pred\")\n",
    "        plt.show()\n",
    "\n",
    "    return cf, nca, r, ar, fm, mi, nmi, ami, a\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/framework\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd()) # run to check current working directory and update file path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_collections = {\n",
    "    # \"g2mg\": [\n",
    "    #     \"g2mg_128_10\",\n",
    "    #     \"g2mg_128_20\",\n",
    "    #     \"g2mg_128_30\",\n",
    "    #     \"g2mg_128_40\",\n",
    "    #     \"g2mg_128_50\",\n",
    "    #     \"g2mg_128_60\",\n",
    "    #     \"g2mg_128_70\",\n",
    "    #     \"g2mg_128_80\",\n",
    "    #     \"g2mg_128_90\",\n",
    "    #     \"g2mg_16_10\",\n",
    "    #     \"g2mg_16_20\",\n",
    "    #     \"g2mg_16_30\",\n",
    "    #     \"g2mg_16_40\",\n",
    "    #     \"g2mg_16_50\",\n",
    "    #     \"g2mg_16_60\",\n",
    "    #     \"g2mg_16_70\",\n",
    "    #     \"g2mg_16_80\",\n",
    "    #     \"g2mg_16_90\",\n",
    "    #     \"g2mg_1_10\",\n",
    "    #     \"g2mg_1_20\",\n",
    "    #     \"g2mg_1_30\",\n",
    "    #     \"g2mg_1_40\",\n",
    "    #     \"g2mg_1_50\",\n",
    "    #     \"g2mg_1_60\",\n",
    "    #     \"g2mg_1_70\",\n",
    "    #     \"g2mg_1_80\",\n",
    "    #     \"g2mg_1_90\",\n",
    "    #     \"g2mg_2_10\",\n",
    "    #     \"g2mg_2_20\",\n",
    "    #     \"g2mg_2_30\",\n",
    "    #     \"g2mg_2_40\",\n",
    "    #     \"g2mg_2_50\",\n",
    "    #     \"g2mg_2_60\",\n",
    "    #     \"g2mg_2_70\",\n",
    "    #     \"g2mg_2_80\",\n",
    "    #     \"g2mg_2_90\",\n",
    "    #     \"g2mg_32_10\",\n",
    "    #     \"g2mg_32_20\",\n",
    "    #     \"g2mg_32_30\",\n",
    "    #     \"g2mg_32_40\",\n",
    "    #     \"g2mg_32_50\",\n",
    "    #     \"g2mg_32_60\",\n",
    "    #     \"g2mg_32_70\",\n",
    "    #     \"g2mg_32_80\",\n",
    "    #     \"g2mg_32_90\",\n",
    "    #     \"g2mg_4_10\",\n",
    "    #     \"g2mg_4_20\",\n",
    "    #     \"g2mg_4_30\",\n",
    "    #     \"g2mg_4_40\",\n",
    "    #     \"g2mg_4_50\",\n",
    "    #     \"g2mg_4_60\",\n",
    "    #     \"g2mg_4_70\",\n",
    "    #     \"g2mg_4_80\",\n",
    "    #     \"g2mg_4_90\",\n",
    "    #     \"g2mg_64_10\",\n",
    "    #     \"g2mg_64_20\",\n",
    "    #     \"g2mg_64_30\",\n",
    "    #     \"g2mg_64_40\",\n",
    "    #     \"g2mg_64_50\",\n",
    "    #     \"g2mg_64_60\",\n",
    "    #     \"g2mg_64_70\",\n",
    "    #     \"g2mg_64_80\",\n",
    "    #     \"g2mg_64_90\",\n",
    "    #     \"g2mg_8_10\",\n",
    "    #     \"g2mg_8_20\",\n",
    "    #     \"g2mg_8_30\",\n",
    "    #     \"g2mg_8_40\",\n",
    "    #     \"g2mg_8_50\",\n",
    "    #     \"g2mg_8_60\",\n",
    "    #     \"g2mg_8_70\",\n",
    "    #     \"g2mg_8_80\",\n",
    "    #     \"g2mg_8_90\"\n",
    "    # ],\n",
    "    \"other\": [\n",
    "        \"chameleon_t4_8k\",\n",
    "        \"chameleon_t5_8k\",\n",
    "        \"chameleon_t7_10k\",\n",
    "        \"chameleon_t8_8k\",\n",
    "        \"hdbscan\",\n",
    "        \"iris\",\n",
    "        \"iris5\",\n",
    "        \"square\"\n",
    "    ],\n",
    "    \"graves\": [\n",
    "        \"dense\",\n",
    "        \"fuzzyx\",\n",
    "        \"line\",\n",
    "        \"parabolic\",\n",
    "        \"ring\",\n",
    "        \"ring_noisy\",\n",
    "        \"ring_outliers\",\n",
    "        \"zigzag\",\n",
    "        \"zigzag_noisy\",\n",
    "        \"zigzag_outliers\"\n",
    "    ],\n",
    "    \"sipu\": [\n",
    "        \"a1\",\n",
    "        \"a2\",\n",
    "        \"a3\",\n",
    "        \"aggregation\",\n",
    "        \"birch1\",\n",
    "        \"birch2\",\n",
    "        \"compound\",\n",
    "        \"d31\",\n",
    "        \"flame\",\n",
    "        \"jain\",\n",
    "        \"pathbased\",\n",
    "        \"r15\",\n",
    "        \"s1\",\n",
    "        \"s2\",\n",
    "        \"s3\",\n",
    "        \"s4\",\n",
    "        \"spiral\",\n",
    "        \"unbalance\",\n",
    "        \"worms_2\",\n",
    "        \"worms_64\"\n",
    "    ],\n",
    "    \"mnist\": [\n",
    "        \"digits\",\n",
    "        \"fashion\"\n",
    "    ],\n",
    " \n",
    "    \"wut\": [\n",
    "        \"circles\",\n",
    "        \"cross\",\n",
    "        \"graph\",\n",
    "        \"isolation\",\n",
    "        \"labirynth\",\n",
    "        \"mk1\",\n",
    "        \"mk2\",\n",
    "        \"mk3\",\n",
    "        \"mk4\",\n",
    "        \"olympic\",\n",
    "        \"smile\",\n",
    "        \"stripes\",\n",
    "        \"trajectories\",\n",
    "        \"trapped_lovers\",\n",
    "        \"twosplashes\",\n",
    "        \"windows\",\n",
    "        \"x1\",\n",
    "        \"x2\",\n",
    "        \"x3\",\n",
    "        \"z1\",\n",
    "        \"z2\",\n",
    "        \"z3\"\n",
    "    ],\n",
    "    \"fcps\": [\n",
    "        \"atom\",\n",
    "        \"chainlink\",\n",
    "        \"engytime\",\n",
    "        \"hepta\",\n",
    "        \"lsun\",\n",
    "        \"target\",\n",
    "        \"tetra\",\n",
    "        \"twodiamonds\",\n",
    "        \"wingnut\"\n",
    "    ],\n",
    "    \"uci\": [\n",
    "        \"ecoli\",\n",
    "        \"glass\",\n",
    "        \"ionosphere\",\n",
    "        \"sonar\",\n",
    "        \"statlog\",\n",
    "        \"wdbc\",\n",
    "        \"wine\",\n",
    "        \"yeast\"\n",
    "    ],\n",
    "    # \"h2mg\": [\n",
    "    #     \"h2mg_128_10\",\n",
    "    #     \"h2mg_128_20\",\n",
    "    #     \"h2mg_128_30\",\n",
    "    #     \"h2mg_128_40\",\n",
    "    #     \"h2mg_128_50\",\n",
    "    #     \"h2mg_128_60\",\n",
    "    #     \"h2mg_128_70\",\n",
    "    #     \"h2mg_128_80\",\n",
    "    #     \"h2mg_128_90\",\n",
    "    #     \"h2mg_16_10\",\n",
    "    #     \"h2mg_16_20\",\n",
    "    #     \"h2mg_16_30\",\n",
    "    #     \"h2mg_16_40\",\n",
    "    #     \"h2mg_16_50\",\n",
    "    #     \"h2mg_16_60\",\n",
    "    #     \"h2mg_16_70\",\n",
    "    #     \"h2mg_16_80\",\n",
    "    #     \"h2mg_16_90\",\n",
    "    #     \"h2mg_1_10\",\n",
    "    #     \"h2mg_1_20\",\n",
    "    #     \"h2mg_1_30\",\n",
    "    #     \"h2mg_1_40\",\n",
    "    #     \"h2mg_1_50\",\n",
    "    #     \"h2mg_1_60\",\n",
    "    #     \"h2mg_1_70\",\n",
    "    #     \"h2mg_1_80\",\n",
    "    #     \"h2mg_1_90\",\n",
    "    #     \"h2mg_2_10\",\n",
    "    #     \"h2mg_2_20\",\n",
    "    #     \"h2mg_2_30\",\n",
    "    #     \"h2mg_2_40\",\n",
    "    #     \"h2mg_2_50\",\n",
    "    #     \"h2mg_2_60\",\n",
    "    #     \"h2mg_2_70\",\n",
    "    #     \"h2mg_2_80\",\n",
    "    #     \"h2mg_2_90\",\n",
    "    #     \"h2mg_32_10\",\n",
    "    #     \"h2mg_32_20\",\n",
    "    #     \"h2mg_32_30\",\n",
    "    #     \"h2mg_32_40\",\n",
    "    #     \"h2mg_32_50\",\n",
    "    #     \"h2mg_32_60\",\n",
    "    #     \"h2mg_32_70\",\n",
    "    #     \"h2mg_32_80\",\n",
    "    #     \"h2mg_32_90\",\n",
    "    #     \"h2mg_4_10\",\n",
    "    #     \"h2mg_4_20\",\n",
    "    #     \"h2mg_4_30\",\n",
    "    #     \"h2mg_4_40\",\n",
    "    #     \"h2mg_4_50\",\n",
    "    #     \"h2mg_4_60\",\n",
    "    #     \"h2mg_4_70\",\n",
    "    #     \"h2mg_4_80\",\n",
    "    #     \"h2mg_4_90\",\n",
    "    #     \"h2mg_64_10\",\n",
    "    #     \"h2mg_64_20\",\n",
    "    #     \"h2mg_64_30\",\n",
    "    #     \"h2mg_64_40\",\n",
    "    #     \"h2mg_64_50\",\n",
    "    #     \"h2mg_64_60\",\n",
    "    #     \"h2mg_64_70\",\n",
    "    #     \"h2mg_64_80\",\n",
    "    #     \"h2mg_64_90\",\n",
    "    #     \"h2mg_8_10\",\n",
    "    #     \"h2mg_8_20\",\n",
    "    #     \"h2mg_8_30\",\n",
    "    #     \"h2mg_8_40\",\n",
    "    #     \"h2mg_8_50\",\n",
    "    #     \"h2mg_8_60\",\n",
    "    #     \"h2mg_8_70\",\n",
    "    #     \"h2mg_8_80\",\n",
    "    #     \"h2mg_8_90\"\n",
    "    # ]\n",
    "}\n",
    "clustering_methods = [\"genie\", \"kmeans\", \"agglomerative\", \"spectral\",\"gaussianmixture\", \"hdbscan\", \"kmodes\", \"birch\", \"minibatchkmeans\" ,\"kmedoids\", \"latentdirichletallocation\", \"spectralcoclustering\", \"bayesiangaussianmixture\"]\n",
    "# clustering_methods = [\"genie\"]\n",
    "result_csv = \"/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/framework/results/josh.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Run to set the column names for the csv file\n",
    "\"\"\"\n",
    "import os\n",
    "import csv\n",
    "\n",
    "if os.path.exists(result_csv):\n",
    "    print(\"File already exists\")\n",
    "else:\n",
    "    try:\n",
    "        with open(result_csv, mode='w', newline='') as file: \n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                \"Collection\", \"Dataset\", \"Clustering Method\", \"Label\", \"Embedding\",\n",
    "                \"Original Dimensions\", \"Embedding Dimensions\",\n",
    "                \"CF\", \"NCA Score\", \"R\", \"AR\", \"FM\", \"MI\", \"NMI\", \"AMI\", \"A\"\n",
    "            ])\n",
    "    except Exception as e:\n",
    "        print(\"Error writing to file: \", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe create a cache or temporary storage for the embeddings\n",
    "# TODO: parallelize the embedding and clustering process per dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: other, Dataset: chameleon_t4_8k\n",
      "Loaded:  8000  | Dimension:  2  | Label count:  1\n",
      "Getting embeddings (with caching)...\n",
      "Cache not found. Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name BAAI/bge-reranker-large. Creating a new one with mean pooling.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at BAAI/bge-reranker-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "with open(result_csv, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for collection, datasets in eval_collections.items():\n",
    "        for dataset in datasets:\n",
    "            print(f\"Collection: {collection}, Dataset: {dataset}\")\n",
    "            X, benchmark, X_embedded_dict = load_data(collection, dataset)\n",
    "            \n",
    "            # Get the original data dimensions (e.g., \"150 x 4\")\n",
    "            orig_dim = f\"{X.shape[0]} x {X.shape[1]}\"\n",
    "            \n",
    "            for label in range(0, len(benchmark.labels)):\n",
    "                for embedding_technique, embedded_data in X_embedded_dict.items():\n",
    "                    # Determine the embedding dimensions if available\n",
    "                    if hasattr(embedded_data, \"shape\"):\n",
    "                        embed_dim = f\"{embedded_data.shape[0]} x {embedded_data.shape[1]}\"\n",
    "                    else:\n",
    "                        embed_dim = \"Unknown\"\n",
    "                    umap_model = umap.UMAP(\n",
    "                        n_components=2,\n",
    "                        n_neighbors=15,\n",
    "                        min_dist=0.1,\n",
    "                        metric='euclidean',\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    umap_embedded_data = umap_model.fit_transform(embedded_data)\n",
    "                    \n",
    "                    for clustering_method in clustering_methods:\n",
    "                        cf, nca_score, r, ar, fm, mi, nmi, ami, a = predict(\n",
    "                            embedding_technique, \n",
    "                            umap_embedded_data, \n",
    "                            label, \n",
    "                            benchmark, \n",
    "                            clustering_method\n",
    "                        )\n",
    "                        print(cf)\n",
    "                        cf_str = \", \".join(map(str, cf.flatten())) if hasattr(cf, \"flatten\") else \", \".join(map(str, cf))\n",
    "    \n",
    "                        writer.writerow([\n",
    "                            collection,          # Collection\n",
    "                            dataset,             # Dataset\n",
    "                            clustering_method,   # Clustering Method\n",
    "                            label,               # Label index\n",
    "                            embedding_technique, # Embedding technique used\n",
    "                            orig_dim,            # Original data dimensions\n",
    "                            embed_dim,           # Embedding dimensions\n",
    "                            f\"cf: {cf} \",        # Confusion matrix\n",
    "                            f\"nca: {nca_score} \",# Normalized Clustering Accuracy\n",
    "                            f\"r: {r} \",         # Rand index\n",
    "                            f\"ar: {ar} \",       # Adjusted Rand index\n",
    "                            f\"fm: {fm} \",       # Fowlkes-Mallows index\n",
    "                            f\"mi: {mi} \",       # Mutual Information\n",
    "                            f\"nmi: {nmi} \",     # Normalized Mutual Information\n",
    "                            f\"ami: {ami} \",     # Adjusted Mutual Information\n",
    "                            f\"a: {a} \"         # Accuracy Score\n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_and_compare_csv(file_path):\n",
    "    # Define column names based on the CSV structure\n",
    "    col_names = [\n",
    "        \"Collection\", \"Dataset\", \"Clustering Method\", \"Label\", \"Embedding\", \"Original Dimensions\", \"Embedding Dimensions\",\n",
    "        \"CF\", \"NCA\", \"r\", \"ar\", \"fm\", \"mi\", \"nmi\", \"ami\", \"a\"\n",
    "    ]\n",
    "    \n",
    "    # Read CSV without a header, assigning our own column names\n",
    "    df = pd.read_csv(file_path, header=None, names=col_names)\n",
    "    \n",
    "    # Remove extra whitespace from all string cells\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    \n",
    "    # Function to clean a numeric field with a prefix\n",
    "    def clean_numeric(value, prefix):\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(prefix, \"\").strip()\n",
    "        try:\n",
    "            return float(value)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # Clean the NCA column by removing the \"nca:\" prefix and converting to float\n",
    "    df['NCA'] = df['NCA'].apply(lambda x: clean_numeric(x, 'nca:'))\n",
    "    \n",
    "    # Group by the specified columns\n",
    "    grouped = df.groupby([\"Collection\", \"Dataset\", \"Clustering Method\", \"Label\"])\n",
    "    \n",
    "    filtered_rows = []\n",
    "    \n",
    "    # Iterate over each group\n",
    "    for name, group in grouped:\n",
    "        # Find the \"Base\" row in the Embedding column\n",
    "        base_row = group[group['Embedding'] == 'Base']\n",
    "        if not base_row.empty:\n",
    "            base_value = base_row.iloc[0]['NCA']\n",
    "            base_row_list = base_row.iloc[0].tolist()\n",
    "            base_added = False\n",
    "            \n",
    "            # Compare each row's NCA value to the base_value\n",
    "            for index, row in group.iterrows():\n",
    "                if row['Embedding'] != 'Base' and row['NCA'] > base_value:\n",
    "                    if not base_added:\n",
    "                        filtered_rows.append(base_row_list)\n",
    "                        base_added = True\n",
    "                    filtered_rows.append(row.tolist())\n",
    "    \n",
    "    # Create a new DataFrame from the filtered rows and remove duplicates\n",
    "    filtered_df = pd.DataFrame(filtered_rows, columns=df.columns)\n",
    "    filtered_df = filtered_df.drop_duplicates()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Example usage\n",
    "file_path = \"/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/framework/results/josh.csv\"\n",
    "filtered_df = filter_and_compare_csv(file_path)\n",
    "\n",
    "# Select only the desired columns for display\n",
    "display_columns = [\"Collection\", \"Dataset\", \"Clustering Method\", \"Label\", \"NCA\"]\n",
    "print(filtered_df[display_columns])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
