{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clustbench'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mclustbench\u001b[39;00m \u001b[38;5;66;03m# clustering-benchmarks\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgenieclust\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;66;03m# we will need these later\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clustbench'"
     ]
    }
   ],
   "source": [
    "import clustbench # clustering-benchmarks\n",
    "import os.path, genieclust, sklearn.cluster # we will need these later\n",
    "import matplotlib.pyplot as plt, numpy as np, pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def generate_embeddings(X):\n",
    "# #     from sklearn.manifold import TSNE\n",
    "# #     from sklearn.decomposition import PCA\n",
    "\n",
    "# #     def apply_embedding(model, data):\n",
    "# #         embedding = model.fit_transform(data)\n",
    "# #         return embedding\n",
    "\n",
    "# #     # Example usage with PCA\n",
    "# #     pca_model = PCA(n_components=2)\n",
    "# #     X_pca = apply_embedding(pca_model, X)\n",
    "# #     print(\"PCA Result:\\n\", X_pca[:5])\n",
    "\n",
    "# #     # Example usage with t-SNE\n",
    "# #     tsne_model = TSNE(n_components=2, random_state=42)\n",
    "# #     X_tsne = apply_embedding(tsne_model, X)\n",
    "# #     print(\"t-SNE Result:\\n\", X_tsne[:5])\n",
    "\n",
    "# #     X_embedded_dict = {\"Base\": X, \"PCA\": X_pca, \"t-SNE\": X_tsne}\n",
    "# #     return X_embedded_dict\n",
    "\n",
    "# def generate_embeddings(X, is_text=False):\n",
    "#     \"\"\"\n",
    "#     Generate a dictionary of embeddings from the input data X.\n",
    "    \n",
    "#     For numerical data (is_text=False), a number of dimensionality reduction\n",
    "#     techniques are applied: PCA, t-SNE, UMAP, MDS, Isomap, Locally Linear Embedding,\n",
    "#     Spectral Embedding, Kernel PCA, and a simple autoencoder.\n",
    "    \n",
    "#     For text data (is_text=True), several text embedding techniques are used:\n",
    "#     TF-IDF, CountVectorizer, SentenceTransformer, spaCy vectors, average Word2Vec,\n",
    "#     and Doc2Vec.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     X : array-like or list of str\n",
    "#         If is_text is False, X should be a 2D numerical array of shape (n_samples, n_features).\n",
    "#         If is_text is True, X should be a list (or iterable) of strings.\n",
    "    \n",
    "#     is_text : bool, optional (default=False)\n",
    "#         Set True if X is text data.\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     dict\n",
    "#         A dictionary where the key \"Base\" corresponds to the original data,\n",
    "#         and additional keys correspond to the transformed/embedded representations.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     X_embedded_dict = {\"Base\": X}\n",
    "    \n",
    "#     if not is_text:\n",
    "#         # ---------------------\n",
    "#         # Numeric Data Embeddings\n",
    "#         # ---------------------\n",
    "#         from sklearn.decomposition import PCA, KernelPCA\n",
    "#         from sklearn.manifold import TSNE, MDS, Isomap, LocallyLinearEmbedding, SpectralEmbedding\n",
    "\n",
    "#         # PCA\n",
    "#         pca_model = PCA(n_components=2)\n",
    "#         X_embedded_dict[\"PCA\"] = pca_model.fit_transform(X)\n",
    "#         print(\"PCA embedding done.\")\n",
    "        \n",
    "#         # t-SNE\n",
    "#         tsne_model = TSNE(n_components=2, random_state=42)\n",
    "#         X_embedded_dict[\"t-SNE\"] = tsne_model.fit_transform(X)\n",
    "#         print(\"t-SNE embedding done.\")\n",
    "        \n",
    "#         # UMAP (requires umap-learn package)\n",
    "#         try:\n",
    "#             import umap\n",
    "#             umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "#             X_embedded_dict[\"UMAP\"] = umap_model.fit_transform(X)\n",
    "#             print(\"UMAP embedding done.\")\n",
    "#         except Exception as e:\n",
    "#             print(\"UMAP embedding failed:\", e)\n",
    "        \n",
    "#         # MDS\n",
    "#         mds_model = MDS(n_components=2, random_state=42)\n",
    "#         X_embedded_dict[\"MDS\"] = mds_model.fit_transform(X)\n",
    "#         print(\"MDS embedding done.\")\n",
    "        \n",
    "#         # Isomap\n",
    "#         isomap_model = Isomap(n_components=2)\n",
    "#         X_embedded_dict[\"Isomap\"] = isomap_model.fit_transform(X)\n",
    "#         print(\"Isomap embedding done.\")\n",
    "        \n",
    "#         # Locally Linear Embedding (LLE)\n",
    "#         lle_model = LocallyLinearEmbedding(n_components=2)\n",
    "#         X_embedded_dict[\"LLE\"] = lle_model.fit_transform(X)\n",
    "#         print(\"LLE embedding done.\")\n",
    "        \n",
    "#         # Spectral Embedding\n",
    "#         spectral_model = SpectralEmbedding(n_components=2, random_state=42)\n",
    "#         X_embedded_dict[\"Spectral\"] = spectral_model.fit_transform(X)\n",
    "#         print(\"Spectral embedding done.\")\n",
    "        \n",
    "#         # Kernel PCA with RBF kernel\n",
    "#         kpca_model = KernelPCA(n_components=2, kernel='rbf', random_state=42)\n",
    "#         X_embedded_dict[\"KernelPCA\"] = kpca_model.fit_transform(X)\n",
    "#         print(\"Kernel PCA embedding done.\")\n",
    "        \n",
    "#         # Autoencoder embedding (using TensorFlow/Keras)\n",
    "#         try:\n",
    "#             import tensorflow as tf\n",
    "#             from tensorflow.keras.layers import Input, Dense\n",
    "#             from tensorflow.keras.models import Model\n",
    "            \n",
    "#             input_dim = X.shape[1]\n",
    "#             encoding_dim = 2  # target embedding dimension\n",
    "            \n",
    "#             # Define a simple autoencoder architecture\n",
    "#             input_layer = Input(shape=(input_dim,))\n",
    "#             encoded = Dense(64, activation='relu')(input_layer)\n",
    "#             encoded = Dense(32, activation='relu')(encoded)\n",
    "#             bottleneck = Dense(encoding_dim, activation='linear')(encoded)\n",
    "#             decoded = Dense(32, activation='relu')(bottleneck)\n",
    "#             decoded = Dense(64, activation='relu')(decoded)\n",
    "#             output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "            \n",
    "#             autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#             autoencoder.compile(optimizer='adam', loss='mse')\n",
    "            \n",
    "#             # Train the autoencoder briefly (adjust epochs/batch_size as needed)\n",
    "#             autoencoder.fit(X, X, epochs=50, batch_size=32, verbose=0)\n",
    "#             encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
    "#             X_embedded_dict[\"Autoencoder\"] = encoder.predict(X)\n",
    "#             print(\"Autoencoder embedding done.\")\n",
    "#         except Exception as e:\n",
    "#             print(\"Autoencoder embedding failed:\", e)\n",
    "    \n",
    "#     else:\n",
    "#         # ---------------------\n",
    "#         # Text Data Embeddings\n",
    "#         # ---------------------\n",
    "#         # TF-IDF Vectorizer\n",
    "#         from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#         tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "#         X_tfidf = tfidf_vectorizer.fit_transform(X).toarray()\n",
    "#         X_embedded_dict[\"TF-IDF\"] = X_tfidf\n",
    "#         print(\"TF-IDF embedding done.\")\n",
    "        \n",
    "#         # Count Vectorizer\n",
    "#         count_vectorizer = CountVectorizer(max_features=1000)\n",
    "#         X_count = count_vectorizer.fit_transform(X).toarray()\n",
    "#         X_embedded_dict[\"Count\"] = X_count\n",
    "#         print(\"Count vector embedding done.\")\n",
    "        \n",
    "#         # Sentence Transformer embedding (requires the sentence-transformers package)\n",
    "#         try:\n",
    "#             from sentence_transformers import SentenceTransformer\n",
    "#             sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#             X_sentence = sentence_model.encode(X, show_progress_bar=False)\n",
    "#             X_embedded_dict[\"SentenceTransformer\"] = X_sentence\n",
    "#             print(\"SentenceTransformer embedding done.\")\n",
    "#         except Exception as e:\n",
    "#             print(\"SentenceTransformer embedding failed:\", e)\n",
    "        \n",
    "#         # spaCy embeddings (requires spaCy and a medium/large English model)\n",
    "#         try:\n",
    "#             import spacy\n",
    "#             nlp = spacy.load(\"en_core_web_md\")\n",
    "#             import numpy as np\n",
    "#             X_spacy = [doc.vector for doc in nlp.pipe(X, batch_size=50)]\n",
    "#             X_embedded_dict[\"spaCy\"] = np.array(X_spacy)\n",
    "#             print(\"spaCy embedding done.\")\n",
    "#         except Exception as e:\n",
    "#             print(\"spaCy embedding failed:\", e)\n",
    "        \n",
    "#         # Average Word2Vec embedding using pre-trained GloVe vectors via gensim\n",
    "#         try:\n",
    "#             import gensim.downloader as api\n",
    "#             model = api.load(\"glove-wiki-gigaword-50\")\n",
    "#             import numpy as np\n",
    "#             def avg_word2vec(sentence):\n",
    "#                 words = sentence.split()\n",
    "#                 vectors = [model[word] for word in words if word in model]\n",
    "#                 if vectors:\n",
    "#                     return np.mean(vectors, axis=0)\n",
    "#                 else:\n",
    "#                     return np.zeros(model.vector_size)\n",
    "#             X_word2vec = np.array([avg_word2vec(text) for text in X])\n",
    "#             X_embedded_dict[\"Word2Vec\"] = X_word2vec\n",
    "#             print(\"Average Word2Vec embedding done.\")\n",
    "#         except Exception as e:\n",
    "#             print(\"Word2Vec embedding failed:\", e)\n",
    "        \n",
    "#         # Doc2Vec embedding using gensim (requires training on your corpus)\n",
    "#         try:\n",
    "#             from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#             import numpy as np\n",
    "#             documents = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(X)]\n",
    "#             doc2vec_model = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "#             doc2vec_model.build_vocab(documents)\n",
    "#             doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "#             X_doc2vec = np.array([doc2vec_model.infer_vector(doc.words) for doc in documents])\n",
    "#             X_embedded_dict[\"Doc2Vec\"] = X_doc2vec\n",
    "#             print(\"Doc2Vec embedding done.\")\n",
    "#         except Exception as e:\n",
    "#             print(\"Doc2Vec embedding failed:\", e)\n",
    "    \n",
    "#     return X_embedded_dict\n",
    "\n",
    "# # --- Example usage ---\n",
    "\n",
    "# # For numerical data:\n",
    "# # import numpy as np\n",
    "# # X_numeric = np.random.rand(100, 10)  # 100 samples, 10 features\n",
    "# # embeddings_numeric = generate_embeddings(X_numeric, is_text=False)\n",
    "# # print({k: v.shape for k, v in embeddings_numeric.items()})\n",
    "\n",
    "# # For text data:\n",
    "# # texts = [\n",
    "# #     \"This is the first document.\",\n",
    "# #     \"This document is the second document.\",\n",
    "# #     \"And this is the third one.\",\n",
    "# #     \"Is this the first document?\"\n",
    "# # ]\n",
    "# # embeddings_text = generate_embeddings(texts, is_text=True)\n",
    "# # print({k: v.shape for k, v in embeddings_text.items() if hasattr(v, 'shape')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(X):\n",
    "    \"\"\"\n",
    "    Generate a dictionary of embeddings from a multidimensional NumPy array of numbers.\n",
    "\n",
    "    This function applies both traditional numerical embeddings and text-based embedding\n",
    "    techniques (by converting numeric rows to strings) to produce a variety of representations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        A 2D array of shape (n_samples, n_features) containing numeric data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary where the key \"Base\" corresponds to the original data and additional\n",
    "        keys correspond to various embedded representations.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    embeddings = {\"Base\": X}\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1. Traditional Numerical Embeddings\n",
    "    # ------------------------------------------------\n",
    "    try:\n",
    "        from sklearn.decomposition import (\n",
    "            PCA, KernelPCA, TruncatedSVD, FastICA, FactorAnalysis\n",
    "        )\n",
    "        from sklearn.manifold import (\n",
    "            TSNE, MDS, Isomap, LocallyLinearEmbedding, SpectralEmbedding\n",
    "        )\n",
    "        from sklearn.random_projection import GaussianRandomProjection\n",
    "        \n",
    "        # PCA\n",
    "        # Common default: n_components=2 for visualization; whiten=False (unless needed)\n",
    "        pca_model = PCA(n_components=2, whiten=False, random_state=42)\n",
    "        embeddings[\"PCA\"] = pca_model.fit_transform(X)\n",
    "        print(\"PCA embedding done.\")\n",
    "\n",
    "        # t-SNE\n",
    "        # Common defaults: n_components=2, perplexity=30, learning_rate='auto', n_iter=1000+\n",
    "        # Note: t-SNE can be slow on large datasets. \n",
    "        tsne_model = TSNE(\n",
    "            n_components=2,\n",
    "            perplexity=30,\n",
    "            learning_rate='auto',\n",
    "            n_iter=1000,\n",
    "            random_state=42,\n",
    "            init='pca'  # often helps with convergence\n",
    "        )\n",
    "        embeddings[\"t-SNE\"] = tsne_model.fit_transform(X)\n",
    "        print(\"t-SNE embedding done.\")\n",
    "\n",
    "        # UMAP (requires umap-learn)\n",
    "        # Common defaults: n_neighbors=15, min_dist=0.1, metric='euclidean'\n",
    "        # NOTE: This can be slow for large data. Increase n_epochs or do PCA first if needed.\n",
    "        try:\n",
    "            import umap\n",
    "            umap_model = umap.UMAP(\n",
    "                n_components=2,\n",
    "                n_neighbors=15,\n",
    "                min_dist=0.1,\n",
    "                metric='euclidean',\n",
    "                random_state=42\n",
    "            )\n",
    "            embeddings[\"UMAP\"] = umap_model.fit_transform(X)\n",
    "            print(\"UMAP embedding done.\")\n",
    "        except Exception as e:\n",
    "            print(\"UMAP embedding failed:\", e)\n",
    "\n",
    "        # MDS\n",
    "        # Default: n_components=2, metric=True (classical MDS), can be slow for large data\n",
    "        mds_model = MDS(n_components=2, metric=True, random_state=42, n_init=4, max_iter=300)\n",
    "        embeddings[\"MDS\"] = mds_model.fit_transform(X)\n",
    "        print(\"MDS embedding done.\")\n",
    "\n",
    "        # Isomap\n",
    "        # Default: n_neighbors=5, n_components=2\n",
    "        isomap_model = Isomap(n_components=2, n_neighbors=5)\n",
    "        embeddings[\"Isomap\"] = isomap_model.fit_transform(X)\n",
    "        print(\"Isomap embedding done.\")\n",
    "\n",
    "        # Locally Linear Embedding (LLE)\n",
    "        # Default: n_neighbors=10, n_components=2\n",
    "        lle_model = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "        embeddings[\"LLE\"] = lle_model.fit_transform(X)\n",
    "        print(\"LLE embedding done.\")\n",
    "\n",
    "        # Spectral Embedding\n",
    "        # Default: n_components=2, affinity='nearest_neighbors', n_neighbors=5\n",
    "        spectral_model = SpectralEmbedding(\n",
    "            n_components=2,\n",
    "            n_neighbors=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        embeddings[\"Spectral\"] = spectral_model.fit_transform(X)\n",
    "        print(\"Spectral embedding done.\")\n",
    "\n",
    "        # Kernel PCA with RBF kernel\n",
    "        # Common defaults: n_components=2, kernel='rbf', gamma=None (auto)\n",
    "        kpca_model = KernelPCA(\n",
    "            n_components=2,\n",
    "            kernel='rbf',\n",
    "            gamma=None,\n",
    "            random_state=42\n",
    "        )\n",
    "        embeddings[\"KernelPCA\"] = kpca_model.fit_transform(X)\n",
    "        print(\"Kernel PCA embedding done.\")\n",
    "\n",
    "        # Autoencoder embedding (using TensorFlow/Keras)\n",
    "        # Basic architecture: input->(64)->(32)->(2)->(32)->(64)->output\n",
    "        # Epochs, batch_size can be tuned for better performance\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            from tensorflow.keras.layers import Input, Dense\n",
    "            from tensorflow.keras.models import Model\n",
    "\n",
    "            input_dim = X.shape[1]\n",
    "            encoding_dim = 2  # target dimension\n",
    "            input_layer = Input(shape=(input_dim,))\n",
    "            encoded = Dense(64, activation='relu')(input_layer)\n",
    "            encoded = Dense(32, activation='relu')(encoded)\n",
    "            bottleneck = Dense(encoding_dim, activation='linear')(encoded)\n",
    "            decoded = Dense(32, activation='relu')(bottleneck)\n",
    "            decoded = Dense(64, activation='relu')(decoded)\n",
    "            output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "            autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "            autoencoder.compile(optimizer='adam', loss='mse')\n",
    "            # Train briefly (increase epochs for better results on real data)\n",
    "            autoencoder.fit(X, X, epochs=50, batch_size=32, verbose=0)\n",
    "            encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
    "            embeddings[\"Autoencoder\"] = encoder.predict(X)\n",
    "            print(\"Autoencoder embedding done.\")\n",
    "        except Exception as e:\n",
    "            print(\"Autoencoder embedding failed:\", e)\n",
    "\n",
    "        # Random Projection\n",
    "        # Common default: n_components=2, use GaussianRandomProjection\n",
    "        rp = GaussianRandomProjection(n_components=2, eps=0.1, random_state=42)\n",
    "        embeddings[\"RandomProjection\"] = rp.fit_transform(X)\n",
    "        print(\"Random Projection embedding done.\")\n",
    "\n",
    "        # Truncated SVD\n",
    "        # Common default: n_components=2, good for sparse data (like TF-IDF)\n",
    "        svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "        embeddings[\"TruncatedSVD\"] = svd.fit_transform(X)\n",
    "        print(\"Truncated SVD embedding done.\")\n",
    "\n",
    "        # FastICA\n",
    "        # Common defaults: n_components=2, whiten=True, max_iter=200\n",
    "        ica = FastICA(n_components=2, whiten=True, max_iter=200, random_state=42)\n",
    "        embeddings[\"FastICA\"] = ica.fit_transform(X)\n",
    "        print(\"FastICA embedding done.\")\n",
    "\n",
    "        # Factor Analysis\n",
    "        # Default: n_components=2\n",
    "        fa = FactorAnalysis(n_components=2, random_state=42)\n",
    "        embeddings[\"FactorAnalysis\"] = fa.fit_transform(X)\n",
    "        print(\"Factor Analysis embedding done.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in numerical embeddings:\", e)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. Text-Based Embeddings on Numeric Data\n",
    "    #    (Convert each row to a string, then treat it as text)\n",
    "    # ------------------------------------------------\n",
    "    try:\n",
    "        # Convert each row (sample) to a space-separated string, e.g., \"0.12 0.57 0.99 ...\"\n",
    "        X_as_str = [\" \".join(map(str, row)) for row in X]\n",
    "        print(\"Converted numeric data to strings.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error converting numeric data to strings:\", e)\n",
    "        X_as_str = []\n",
    "\n",
    "    # TF-IDF Vectorizer\n",
    "    # Common defaults: max_features=500 (can raise if large vocabulary)\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "        X_tfidf = tfidf_vectorizer.fit_transform(X_as_str).toarray()\n",
    "        embeddings[\"TF-IDF_str\"] = X_tfidf\n",
    "        print(\"TF-IDF (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"TF-IDF embedding error:\", e)\n",
    "\n",
    "    # Count Vectorizer\n",
    "    # Common defaults: max_features=500\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        count_vectorizer = CountVectorizer(max_features=500)\n",
    "        X_count = count_vectorizer.fit_transform(X_as_str).toarray()\n",
    "        embeddings[\"Count_str\"] = X_count\n",
    "        print(\"Count Vectorizer (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Count Vectorizer embedding error:\", e)\n",
    "\n",
    "    # Character-level TF-IDF\n",
    "    # Common defaults: analyzer='char', ngram_range=(2,4), max_features=500\n",
    "    try:\n",
    "        char_vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(2, 4),\n",
    "            max_features=500\n",
    "        )\n",
    "        X_char_tfidf = char_vectorizer.fit_transform(X_as_str).toarray()\n",
    "        embeddings[\"CharTFIDF_str\"] = X_char_tfidf\n",
    "        print(\"Character-level TF-IDF (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Character-level TF-IDF error:\", e)\n",
    "\n",
    "    # Hashing Vectorizer\n",
    "    # Common defaults: n_features=500\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import HashingVectorizer\n",
    "        hv = HashingVectorizer(n_features=500)\n",
    "        X_hash = hv.transform(X_as_str).toarray()\n",
    "        embeddings[\"Hashing_str\"] = X_hash\n",
    "        print(\"Hashing Vectorizer (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Hashing Vectorizer error:\", e)\n",
    "\n",
    "    # SentenceTransformer embedding (requires sentence-transformers)\n",
    "    # Example model: paraphrase-MiniLM-L6-v2 (small & fast)\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        st_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        X_st = st_model.encode(X_as_str, show_progress_bar=False)\n",
    "        embeddings[\"SentenceTransformer_str\"] = X_st\n",
    "        print(\"SentenceTransformer (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"SentenceTransformer embedding error:\", e)\n",
    "\n",
    "    # DistilBERT embedding via HuggingFace Transformers\n",
    "    # For each row-as-string, tokenize and average the hidden states\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        import torch\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        def get_distilbert_embedding(text):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            # Mean pooling over token embeddings\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()[0]\n",
    "            return embedding\n",
    "\n",
    "        X_distilbert = np.array([get_distilbert_embedding(txt) for txt in X_as_str])\n",
    "        embeddings[\"DistilBERT_str\"] = X_distilbert\n",
    "        print(\"DistilBERT (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"DistilBERT embedding error:\", e)\n",
    "\n",
    "    # Universal Sentence Encoder (USE) via TensorFlow Hub\n",
    "    # Good universal text embedding, some limitations on max length \n",
    "    try:\n",
    "        import tensorflow_hub as hub\n",
    "        use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        X_use = use_model(X_as_str).numpy()\n",
    "        embeddings[\"USE_str\"] = X_use\n",
    "        print(\"Universal Sentence Encoder (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Universal Sentence Encoder embedding error:\", e)\n",
    "\n",
    "    # NMF on the TF-IDF representation (to further reduce dimensionality)\n",
    "    # Common defaults: n_components=2, init='nndsvd'\n",
    "    try:\n",
    "        from sklearn.decomposition import NMF\n",
    "        nmf_model = NMF(n_components=2, init='nndsvd', random_state=42, max_iter=200)\n",
    "        X_nmf = nmf_model.fit_transform(X_tfidf)\n",
    "        embeddings[\"NMF_TFIDF_str\"] = X_nmf\n",
    "        print(\"NMF on TF-IDF (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"NMF embedding error:\", e)\n",
    "\n",
    "    # Doc2Vec embedding using gensim\n",
    "    # vector_size=50, min_count=1, epochs=40 are typical defaults for small data\n",
    "    try:\n",
    "        from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "        documents = [TaggedDocument(words=txt.split(), tags=[str(i)]) for i, txt in enumerate(X_as_str)]\n",
    "        doc2vec_model = Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
    "        doc2vec_model.build_vocab(documents)\n",
    "        doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "        X_doc2vec = np.array([doc2vec_model.infer_vector(txt.split()) for txt in X_as_str])\n",
    "        embeddings[\"Doc2Vec_str\"] = X_doc2vec\n",
    "        print(\"Doc2Vec (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Doc2Vec embedding error:\", e)\n",
    "\n",
    "    # Optional: Add Latent Dirichlet Allocation (LDA) for topic modeling on the string data\n",
    "    # This can be done on the TF-IDF or Count vector for text\n",
    "    try:\n",
    "        from sklearn.decomposition import LatentDirichletAllocation\n",
    "        # For demonstration, let's do LDA with 5 topics on the Count vector\n",
    "        if \"Count_str\" in embeddings:\n",
    "            lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "            X_lda = lda_model.fit_transform(embeddings[\"Count_str\"])\n",
    "            embeddings[\"LDA_Count_str\"] = X_lda\n",
    "            print(\"LDA on Count vector (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"LDA embedding error:\", e)\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/gagolews/clustering-data-v1/raw/v1.1.0\"\n",
    "\n",
    "def load_data(collection, dataset):\n",
    "    benchmark = clustbench.load_dataset(collection, dataset, url=data_url)\n",
    "    X = benchmark.data\n",
    "    print(\"Loaded: \", X.shape[0], \" | Dimension: \", X.shape[1], \" | Label count: \", len(benchmark.labels))\n",
    "    print(\"Generating Embeddings...\")\n",
    "    return X, benchmark, generate_embeddings(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "each dataset can have multiple labels, \n",
    "pick one at a time and that defines your partition size, aka k\n",
    "\n",
    "Overall, Genie returned a clustering quite similar to the reference one. We may consider 107\n",
    "(namely, c11 + c22 + c33 ) out of the 120 input points as correctly grouped. In particular, \n",
    "all the red and green reference points (the 2nd and the 3rd row) have been properly discovered.\n",
    "\n",
    "Normalized Clustering Accuracy (NCA) \n",
    "NCA is the averaged percentage of correctly classified points in each cluster \n",
    "above the perfectly uniform label distribution.\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "def predict(embedding_technique, X, label, benchmark, clustering_method, plot=False):\n",
    "    y_true = benchmark.labels[label] \n",
    "    (k := max(y_true))  # or benchmark.n_clusters[0]\n",
    "    if clustering_method.lower() == \"genie\":\n",
    "        # testing genieclust\n",
    "        g = genieclust.Genie(n_clusters=k)  # using default parameters\n",
    "        (y_pred := g.fit_predict(X) + 1)  # +1 makes cluster IDs in 1..k, not 0..(k-1)\n",
    "        cf = genieclust.compare_partitions.compare_partitions(y_true, y_pred)\n",
    "        print(\"Confusion Matrix:\\n\", cf)\n",
    "        nca_score = genieclust.compare_partitions.normalized_clustering_accuracy(y_true, y_pred)\n",
    "        print(\"Normalized Clustering Accuracy: \", nca_score)\n",
    "        if plot:\n",
    "            plt.subplot(1, 2, 1)\n",
    "            genieclust.plots.plot_scatter(X, labels=y_true-1, axis=\"equal\", title=\"y_true\")\n",
    "            plt.subplot(1, 2, 2)\n",
    "            genieclust.plots.plot_scatter(X, labels=y_pred-1, axis=\"equal\", title=\"y_pred\")\n",
    "            plt.show()\n",
    "    \n",
    "    return cf, nca_score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aaditya/development/embedding_based_clustering_research\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd()) # run to check current working directory and update file path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_collections = {\"wut\": [\"x2\"], \"other\": [\"iris\"]}\n",
    "clustering_methods = [\"genie\"]\n",
    "result_csv = \"framework/results/v1_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Run to set the column names for the csv file\n",
    "\"\"\"\n",
    "import os\n",
    "import csv\n",
    "\n",
    "if os.path.exists(result_csv):\n",
    "    print(\"File already exists\")\n",
    "else:\n",
    "    try:\n",
    "        with open(result_csv, mode='w', newline='') as file: \n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Collection\", \"Dataset\", \"Clustering Method\", \"Label\", \"Embedding\", \"NCA Score\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error writing to file: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe create a cache or temporary storage for the embeddings\n",
    "# TODO: parallelize the embedding and clustering process per dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: wut, Dataset: x2\n",
      "Loaded:  120  | Dimension:  2  | Label count:  2\n",
      "Generating Embeddings...\n",
      "PCA embedding done.\n",
      "t-SNE embedding done.\n",
      "MDS embedding done.\n",
      "Isomap embedding done.\n",
      "LLE embedding done.\n",
      "Spectral embedding done.\n",
      "Kernel PCA embedding done.\n"
     ]
    }
   ],
   "source": [
    "with open(result_csv, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for collection, datasets in eval_collections.items():\n",
    "        for dataset in datasets:\n",
    "            print(f\"Collection: {collection}, Dataset: {dataset}\")\n",
    "            X, benchmark, X_embedded_dict = load_data(collection, dataset)\n",
    "            \n",
    "            for label in range(0, len(benchmark.labels)):\n",
    "                for embedding_technique, embedded_data in X_embedded_dict.items():\n",
    "                    for clustering_method in clustering_methods:\n",
    "                        cf, nca_score = predict(\n",
    "                            embedding_technique, \n",
    "                            embedded_data, \n",
    "                            label, \n",
    "                            benchmark, \n",
    "                            clustering_method\n",
    "                        )\n",
    "                        writer.writerow([\n",
    "                            collection,          # e.g. \"wut\"\n",
    "                            dataset,             # e.g. \"x2\"\n",
    "                            clustering_method,   # e.g. \"genie\"\n",
    "                            label,               # which label set index (0, 1, ...)\n",
    "                            embedding_technique, # e.g. \"PCA\", \"t-SNE\", ...\n",
    "                            cf,                  # confusion matrix\n",
    "                            nca_score            # normalized clustering accuracy\n",
    "                        ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
