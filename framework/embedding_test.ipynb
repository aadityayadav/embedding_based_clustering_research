{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clustbench \n",
    "import os.path, genieclust, sklearn.cluster # we will need these later\n",
    "import matplotlib.pyplot as plt, numpy as np, pandas as pd\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(X):\n",
    "    \"\"\"\n",
    "    Generate a dictionary of embeddings from a multidimensional NumPy array of numbers.\n",
    "\n",
    "    This function applies both traditional numerical embeddings and text-based embedding\n",
    "    techniques (by converting numeric rows to strings) to produce a variety of representations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        A 2D array of shape (n_samples, n_features) containing numeric data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary where the key \"Base\" corresponds to the original data and additional\n",
    "        keys correspond to various embedded representations.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    embeddings = {\"Base\": X}\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1. Traditional Numerical Embeddings\n",
    "    # ------------------------------------------------\n",
    "    try:\n",
    "        from sklearn.decomposition import (\n",
    "            PCA, KernelPCA, TruncatedSVD, FastICA, FactorAnalysis\n",
    "        )\n",
    "        from sklearn.manifold import (\n",
    "            TSNE, MDS, Isomap, LocallyLinearEmbedding, SpectralEmbedding\n",
    "        )\n",
    "        from sklearn.random_projection import GaussianRandomProjection\n",
    "        \n",
    "        # PCA\n",
    "        # Common default: n_components=2 for visualization; whiten=False (unless needed)\n",
    "        pca_model = PCA(n_components=2, whiten=False, random_state=42)\n",
    "        embeddings[\"PCA\"] = pca_model.fit_transform(X)\n",
    "        print(\"PCA embedding done.\")\n",
    "\n",
    "        # t-SNE\n",
    "        # Common defaults: n_components=2, perplexity=30, learning_rate='auto', n_iter=1000+\n",
    "        # Note: t-SNE can be slow on large datasets. \n",
    "        tsne_model = TSNE(\n",
    "            n_components=2,\n",
    "            perplexity=30,\n",
    "            learning_rate='auto',\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            init='pca'  # often helps with convergence\n",
    "        )\n",
    "        embeddings[\"t-SNE\"] = tsne_model.fit_transform(X)\n",
    "        print(\"t-SNE embedding done.\")\n",
    "\n",
    "        # UMAP (requires umap-learn)\n",
    "        # Common defaults: n_neighbors=15, min_dist=0.1, metric='euclidean'\n",
    "        # NOTE: This can be slow for large data. Increase n_epochs or do PCA first if needed.\n",
    "        try:\n",
    "            import umap\n",
    "            umap_model = umap.UMAP(\n",
    "                n_components=2,\n",
    "                n_neighbors=15,\n",
    "                min_dist=0.1,\n",
    "                metric='euclidean',\n",
    "                random_state=42\n",
    "            )\n",
    "            embeddings[\"UMAP\"] = umap_model.fit_transform(X)\n",
    "            print(\"UMAP embedding done.\")\n",
    "        except Exception as e:\n",
    "            print(\"UMAP embedding failed:\", e)\n",
    "\n",
    "        # MDS\n",
    "        # Default: n_components=2, metric=True (classical MDS), can be slow for large data\n",
    "        mds_model = MDS(n_components=2, metric=True, random_state=42, n_init=4, max_iter=300)\n",
    "        embeddings[\"MDS\"] = mds_model.fit_transform(X)\n",
    "        print(\"MDS embedding done.\")\n",
    "\n",
    "        # Isomap\n",
    "        # Default: n_neighbors=5, n_components=2\n",
    "        isomap_model = Isomap(n_components=2, n_neighbors=5)\n",
    "        embeddings[\"Isomap\"] = isomap_model.fit_transform(X)\n",
    "        print(\"Isomap embedding done.\")\n",
    "\n",
    "        # Locally Linear Embedding (LLE)\n",
    "        # Default: n_neighbors=10, n_components=2\n",
    "        lle_model = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "        embeddings[\"LLE\"] = lle_model.fit_transform(X)\n",
    "        print(\"LLE embedding done.\")\n",
    "\n",
    "        # Spectral Embedding\n",
    "        # Default: n_components=2, affinity='nearest_neighbors', n_neighbors=5\n",
    "        spectral_model = SpectralEmbedding(\n",
    "            n_components=2,\n",
    "            n_neighbors=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        embeddings[\"Spectral\"] = spectral_model.fit_transform(X)\n",
    "        print(\"Spectral embedding done.\")\n",
    "\n",
    "        # Kernel PCA with RBF kernel\n",
    "        # Common defaults: n_components=2, kernel='rbf', gamma=None (auto)\n",
    "        kpca_model = KernelPCA(\n",
    "            n_components=2,\n",
    "            kernel='rbf',\n",
    "            gamma=None,\n",
    "            random_state=42\n",
    "        )\n",
    "        embeddings[\"KernelPCA\"] = kpca_model.fit_transform(X)\n",
    "        print(\"Kernel PCA embedding done.\")\n",
    "\n",
    "        # Autoencoder embedding (using TensorFlow/Keras)\n",
    "        # Basic architecture: input->(64)->(32)->(2)->(32)->(64)->output\n",
    "        # Epochs, batch_size can be tuned for better performance\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            from tensorflow.keras.layers import Input, Dense\n",
    "            from tensorflow.keras.models import Model\n",
    "\n",
    "            input_dim = X.shape[1]\n",
    "            encoding_dim = 2  # target dimension\n",
    "            input_layer = Input(shape=(input_dim,))\n",
    "            encoded = Dense(64, activation='relu')(input_layer)\n",
    "            encoded = Dense(32, activation='relu')(encoded)\n",
    "            bottleneck = Dense(encoding_dim, activation='linear')(encoded)\n",
    "            decoded = Dense(32, activation='relu')(bottleneck)\n",
    "            decoded = Dense(64, activation='relu')(decoded)\n",
    "            output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "            autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "            autoencoder.compile(optimizer='adam', loss='mse')\n",
    "            # Train briefly (increase epochs for better results on real data)\n",
    "            autoencoder.fit(X, X, epochs=50, batch_size=32, verbose=0)\n",
    "            encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
    "            embeddings[\"Autoencoder\"] = encoder.predict(X)\n",
    "            print(\"Autoencoder embedding done.\")\n",
    "        except Exception as e:\n",
    "            print(\"Autoencoder embedding failed:\", e)\n",
    "\n",
    "        # Random Projection\n",
    "        # Common default: n_components=2, use GaussianRandomProjection\n",
    "        rp = GaussianRandomProjection(n_components=2, eps=0.1, random_state=42)\n",
    "        embeddings[\"RandomProjection\"] = rp.fit_transform(X)\n",
    "        print(\"Random Projection embedding done.\")\n",
    "\n",
    "        # Truncated SVD\n",
    "        # Common default: n_components=2, good for sparse data (like TF-IDF)\n",
    "        svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "        embeddings[\"TruncatedSVD\"] = svd.fit_transform(X)\n",
    "        print(\"Truncated SVD embedding done.\")\n",
    "\n",
    "        # FastICA\n",
    "        # Common defaults: n_components=2, whiten=True, max_iter=200\n",
    "        ica = FastICA(n_components=2, whiten=True, max_iter=200, random_state=42)\n",
    "        embeddings[\"FastICA\"] = ica.fit_transform(X)\n",
    "        print(\"FastICA embedding done.\")\n",
    "\n",
    "        # Factor Analysis\n",
    "        # Default: n_components=2\n",
    "        fa = FactorAnalysis(n_components=2, random_state=42)\n",
    "        embeddings[\"FactorAnalysis\"] = fa.fit_transform(X)\n",
    "        print(\"Factor Analysis embedding done.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in numerical embeddings:\", e)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. Text-Based Embeddings on Numeric Data\n",
    "    #    (Convert each row to a string, then treat it as text)\n",
    "    # ------------------------------------------------\n",
    "    try:\n",
    "        # Convert each row (sample) to a space-separated string, e.g., \"0.12 0.57 0.99 ...\"\n",
    "        X_as_str = [\" \".join(map(str, row)) for row in X]\n",
    "        print(\"Converted numeric data to strings.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error converting numeric data to strings:\", e)\n",
    "        X_as_str = []\n",
    "\n",
    "    # TF-IDF Vectorizer\n",
    "    # Common defaults: max_features=500 (can raise if large vocabulary)\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "        X_tfidf = tfidf_vectorizer.fit_transform(X_as_str).toarray()\n",
    "        embeddings[\"TF-IDF_str\"] = X_tfidf\n",
    "        print(\"TF-IDF (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"TF-IDF embedding error:\", e)\n",
    "\n",
    "    # Count Vectorizer\n",
    "    # Common defaults: max_features=500\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        count_vectorizer = CountVectorizer(max_features=500)\n",
    "        X_count = count_vectorizer.fit_transform(X_as_str).toarray()\n",
    "        embeddings[\"Count_str\"] = X_count\n",
    "        print(\"Count Vectorizer (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Count Vectorizer embedding error:\", e)\n",
    "\n",
    "    # Character-level TF-IDF\n",
    "    # Common defaults: analyzer='char', ngram_range=(2,4), max_features=500\n",
    "    try:\n",
    "        char_vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(2, 4),\n",
    "            max_features=500\n",
    "        )\n",
    "        X_char_tfidf = char_vectorizer.fit_transform(X_as_str).toarray()\n",
    "        embeddings[\"CharTFIDF_str\"] = X_char_tfidf\n",
    "        print(\"Character-level TF-IDF (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Character-level TF-IDF error:\", e)\n",
    "\n",
    "    # Hashing Vectorizer\n",
    "    # Common defaults: n_features=500\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import HashingVectorizer\n",
    "        hv = HashingVectorizer(n_features=500)\n",
    "        X_hash = hv.transform(X_as_str).toarray()\n",
    "        embeddings[\"Hashing_str\"] = X_hash\n",
    "        print(\"Hashing Vectorizer (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Hashing Vectorizer error:\", e)\n",
    "\n",
    "    # SentenceTransformer embedding (requires sentence-transformers)\n",
    "    # Example model: paraphrase-MiniLM-L6-v2 (small & fast)\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        st_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        X_st = st_model.encode(X_as_str, show_progress_bar=False)\n",
    "        embeddings[\"SentenceTransformer_str\"] = X_st\n",
    "        print(\"SentenceTransformer (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"SentenceTransformer embedding error:\", e)\n",
    "\n",
    "    # DistilBERT embedding via HuggingFace Transformers\n",
    "    # For each row-as-string, tokenize and average the hidden states\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        import torch\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        def get_distilbert_embedding(text):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            # Mean pooling over token embeddings\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()[0]\n",
    "            return embedding\n",
    "\n",
    "        X_distilbert = np.array([get_distilbert_embedding(txt) for txt in X_as_str])\n",
    "        embeddings[\"DistilBERT_str\"] = X_distilbert\n",
    "        print(\"DistilBERT (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"DistilBERT embedding error:\", e)\n",
    "\n",
    "    # Universal Sentence Encoder (USE) via TensorFlow Hub\n",
    "    # Good universal text embedding, some limitations on max length \n",
    "    try:\n",
    "        import tensorflow_hub as hub\n",
    "        use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        X_use = use_model(X_as_str).numpy()\n",
    "        embeddings[\"USE_str\"] = X_use\n",
    "        print(\"Universal Sentence Encoder (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Universal Sentence Encoder embedding error:\", e)\n",
    "\n",
    "    # NMF on the TF-IDF representation (to further reduce dimensionality)\n",
    "    # Common defaults: n_components=2, init='nndsvd'\n",
    "    try:\n",
    "        from sklearn.decomposition import NMF\n",
    "        nmf_model = NMF(n_components=2, init='nndsvd', random_state=42, max_iter=200)\n",
    "        X_nmf = nmf_model.fit_transform(X_tfidf)\n",
    "        embeddings[\"NMF_TFIDF_str\"] = X_nmf\n",
    "        print(\"NMF on TF-IDF (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"NMF embedding error:\", e)\n",
    "\n",
    "    # Doc2Vec embedding using gensim\n",
    "    # vector_size=50, min_count=1, epochs=40 are typical defaults for small data\n",
    "    try:\n",
    "        from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "        documents = [TaggedDocument(words=txt.split(), tags=[str(i)]) for i, txt in enumerate(X_as_str)]\n",
    "        doc2vec_model = Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
    "        doc2vec_model.build_vocab(documents)\n",
    "        doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "        X_doc2vec = np.array([doc2vec_model.infer_vector(txt.split()) for txt in X_as_str])\n",
    "        embeddings[\"Doc2Vec_str\"] = X_doc2vec\n",
    "        print(\"Doc2Vec (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Doc2Vec embedding error:\", e)\n",
    "\n",
    "    # Optional: Add Latent Dirichlet Allocation (LDA) for topic modeling on the string data\n",
    "    # This can be done on the TF-IDF or Count vector for text\n",
    "    try:\n",
    "        from sklearn.decomposition import LatentDirichletAllocation\n",
    "        # For demonstration, let's do LDA with 5 topics on the Count vector\n",
    "        if \"Count_str\" in embeddings:\n",
    "            lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "            X_lda = lda_model.fit_transform(embeddings[\"Count_str\"])\n",
    "            embeddings[\"LDA_Count_str\"] = X_lda\n",
    "            print(\"LDA on Count vector (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"LDA embedding error:\", e)\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/gagolews/clustering-data-v1/raw/v1.1.0\"\n",
    "\n",
    "def load_data(collection, dataset):\n",
    "    benchmark = clustbench.load_dataset(collection, dataset, url=data_url)\n",
    "    X = benchmark.data\n",
    "    print(\"Loaded: \", X.shape[0], \" | Dimension: \", X.shape[1], \" | Label count: \", len(benchmark.labels))\n",
    "    print(\"Generating Embeddings...\")\n",
    "    return X, benchmark, generate_embeddings(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "each dataset can have multiple labels, \n",
    "pick one at a time and that defines your partition size, aka k\n",
    "\n",
    "Overall, Genie returned a clustering quite similar to the reference one. We may consider 107\n",
    "(namely, c11 + c22 + c33 ) out of the 120 input points as correctly grouped. In particular, \n",
    "all the red and green reference points (the 2nd and the 3rd row) have been properly discovered.\n",
    "\n",
    "Normalized Clustering Accuracy (NCA) \n",
    "NCA is the averaged percentage of correctly classified points in each cluster \n",
    "above the perfectly uniform label distribution.\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "def predict(embedding_technique, X, label, benchmark, clustering_method, plot=False):\n",
    "    y_true = benchmark.labels[label] \n",
    "    (k := max(y_true))  # or benchmark.n_clusters[0]\n",
    "    if clustering_method.lower() == \"genie\":\n",
    "        # testing genieclust\n",
    "        g = genieclust.Genie(n_clusters=k)  # using default parameters\n",
    "        (y_pred := g.fit_predict(X) + 1)  # +1 makes cluster IDs in 1..k, not 0..(k-1)\n",
    "        cf = genieclust.compare_partitions.compare_partitions(y_true, y_pred)\n",
    "        print(\"Confusion Matrix:\\n\", cf)\n",
    "        nca_score = genieclust.compare_partitions.normalized_clustering_accuracy(y_true, y_pred)\n",
    "        print(\"Normalized Clustering Accuracy: \", nca_score)\n",
    "        if plot:\n",
    "            plt.subplot(1, 2, 1)\n",
    "            genieclust.plots.plot_scatter(X, labels=y_true-1, axis=\"equal\", title=\"y_true\")\n",
    "            plt.subplot(1, 2, 2)\n",
    "            genieclust.plots.plot_scatter(X, labels=y_pred-1, axis=\"equal\", title=\"y_pred\")\n",
    "            plt.show()\n",
    "    \n",
    "    return cf, nca_score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/framework\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd()) # run to check current working directory and update file path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_collections = {\"wut\": [\"x2\"], \"other\": [\"iris\"]}\n",
    "clustering_methods = [\"genie\"]\n",
    "result_csv = \"/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/framework/results/v1_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Run to set the column names for the csv file\n",
    "\"\"\"\n",
    "import os\n",
    "import csv\n",
    "\n",
    "if os.path.exists(result_csv):\n",
    "    print(\"File already exists\")\n",
    "else:\n",
    "    try:\n",
    "        with open(result_csv, mode='w', newline='') as file: \n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Collection\", \"Dataset\", \"Clustering Method\", \"Label\", \"Embedding\", \"NCA Score\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error writing to file: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe create a cache or temporary storage for the embeddings\n",
    "# TODO: parallelize the embedding and clustering process per dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: wut, Dataset: x2\n",
      "Loaded:  120  | Dimension:  2  | Label count:  2\n",
      "Generating Embeddings...\n",
      "PCA embedding done.\n",
      "t-SNE embedding done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 17:49:26.438569: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/venv/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/venv/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP embedding done.\n",
      "MDS embedding done.\n",
      "Isomap embedding done.\n",
      "LLE embedding done.\n",
      "Spectral embedding done.\n",
      "Kernel PCA embedding done.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Autoencoder embedding done.\n",
      "Random Projection embedding done.\n",
      "Truncated SVD embedding done.\n",
      "Error in numerical embeddings: The 'whiten' parameter of FastICA must be a str among {'unit-variance', 'arbitrary-variance'} or a bool among {False}. Got True instead.\n",
      "Converted numeric data to strings.\n",
      "TF-IDF (from numeric strings) embedding done.\n",
      "Count Vectorizer (from numeric strings) embedding done.\n",
      "Character-level TF-IDF (from numeric strings) embedding done.\n",
      "Hashing Vectorizer (from numeric strings) embedding done.\n",
      "SentenceTransformer (from numeric strings) embedding done.\n",
      "DistilBERT (from numeric strings) embedding done.\n",
      "Universal Sentence Encoder (from numeric strings) embedding done.\n",
      "NMF on TF-IDF (from numeric strings) embedding done.\n",
      "Doc2Vec (from numeric strings) embedding done.\n",
      "LDA on Count vector (from numeric strings) embedding done.\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6882872342370341, 'r': 0.8595238095238096, 'fm': 0.7951855144568276, 'afm': 0.6882935462285218, 'mi': 0.806653849621837, 'nmi': 0.7495519545020478, 'ami': 0.7455077928160847, 'npa': 0.8375000000000001, 'psi': 0.7417149159084644, 'spsi': 0.7384863523573202, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6882872342370341, 'r': 0.8595238095238096, 'fm': 0.7951855144568276, 'afm': 0.6882935462285218, 'mi': 0.806653849621837, 'nmi': 0.7495519545020478, 'ami': 0.7455077928160847, 'npa': 0.8375000000000001, 'psi': 0.7417149159084644, 'spsi': 0.7384863523573202, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6882872342370341, 'r': 0.8595238095238096, 'fm': 0.7951855144568276, 'afm': 0.6882935462285218, 'mi': 0.806653849621837, 'nmi': 0.7495519545020478, 'ami': 0.7455077928160847, 'npa': 0.8375000000000001, 'psi': 0.7417149159084644, 'spsi': 0.7384863523573202, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6882872342370341, 'r': 0.8595238095238096, 'fm': 0.7951855144568276, 'afm': 0.6882935462285218, 'mi': 0.806653849621837, 'nmi': 0.7495519545020478, 'ami': 0.7455077928160847, 'npa': 0.8375000000000001, 'psi': 0.7417149159084644, 'spsi': 0.7384863523573202, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6882872342370341, 'r': 0.8595238095238096, 'fm': 0.7951855144568276, 'afm': 0.6882935462285218, 'mi': 0.806653849621837, 'nmi': 0.7495519545020478, 'ami': 0.7455077928160847, 'npa': 0.8375000000000001, 'psi': 0.7417149159084644, 'spsi': 0.7384863523573202, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6920796257900237, 'r': 0.8623249299719888, 'fm': 0.7959991589147479, 'afm': 0.692168954292608, 'mi': 0.7775513971366752, 'nmi': 0.7170828301076336, 'ami': 0.7125539679584824, 'npa': 0.8375000000000001, 'psi': 0.7192181283514962, 'spsi': 0.7121985815602836, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.4624042187970058, 'r': 0.7497198879551821, 'fm': 0.6600166575366366, 'afm': 0.4640683087093045, 'mi': 0.5682859708811281, 'nmi': 0.5458894294406118, 'ami': 0.53825964352401, 'npa': 0.7000000000000002, 'psi': 0.5359307359307357, 'spsi': 0.5107936507936506, 'nca': 0.6933333333333334}\n",
      "Normalized Clustering Accuracy:  0.6933333333333334\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6882872342370341, 'r': 0.8595238095238096, 'fm': 0.7951855144568276, 'afm': 0.6882935462285218, 'mi': 0.806653849621837, 'nmi': 0.7495519545020478, 'ami': 0.7455077928160847, 'npa': 0.8375000000000001, 'psi': 0.7417149159084644, 'spsi': 0.7384863523573202, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.750803955434529, 'r': 0.888095238095238, 'fm': 0.8356369119975438, 'afm': 0.7508126506268207, 'mi': 0.8460006842666997, 'nmi': 0.7836429729658329, 'ami': 0.780161690164335, 'npa': 0.875, 'psi': 0.7928971615416344, 'spsi': 0.7920342330480579, 'nca': 0.8999999999999999}\n",
      "Normalized Clustering Accuracy:  0.8999999999999999\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6882872342370341, 'r': 0.8595238095238096, 'fm': 0.7951855144568276, 'afm': 0.6882935462285218, 'mi': 0.806653849621837, 'nmi': 0.7495519545020478, 'ami': 0.7455077928160847, 'npa': 0.8375000000000001, 'psi': 0.7417149159084644, 'spsi': 0.7384863523573202, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6882872342370341, 'r': 0.8595238095238096, 'fm': 0.7951855144568276, 'afm': 0.6882935462285218, 'mi': 0.806653849621837, 'nmi': 0.7495519545020478, 'ami': 0.7455077928160847, 'npa': 0.8375000000000001, 'psi': 0.7417149159084644, 'spsi': 0.7384863523573202, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6882872342370341, 'r': 0.8595238095238096, 'fm': 0.7951855144568276, 'afm': 0.6882935462285218, 'mi': 0.806653849621837, 'nmi': 0.7495519545020478, 'ami': 0.7455077928160847, 'npa': 0.8375000000000001, 'psi': 0.7417149159084644, 'spsi': 0.7384863523573202, 'nca': 0.8700000000000001}\n",
      "Normalized Clustering Accuracy:  0.8700000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.010374782193167266, 'r': 0.3589635854341737, 'fm': 0.5806443339832512, 'afm': 0.013748348412881445, 'mi': 0.023533569016151112, 'nmi': 0.04009553301198272, 'ami': 0.009552385284970048, 'npa': 0.13749999999999996, 'psi': 0.009244992295839739, 'spsi': 0.0, 'nca': 0.01666666666666672}\n",
      "Normalized Clustering Accuracy:  0.01666666666666672\n",
      "Confusion Matrix:\n",
      " {'ar': 0.010374782193167266, 'r': 0.3589635854341737, 'fm': 0.5806443339832512, 'afm': 0.013748348412881445, 'mi': 0.023533569016151112, 'nmi': 0.04009553301198272, 'ami': 0.009552385284970048, 'npa': 0.13749999999999996, 'psi': 0.009244992295839739, 'spsi': 0.0, 'nca': 0.01666666666666672}\n",
      "Normalized Clustering Accuracy:  0.01666666666666672\n",
      "Confusion Matrix:\n",
      " {'ar': 0.04777314625981973, 'r': 0.5704481792717087, 'fm': 0.3749802026400403, 'afm': 0.04777424806616972, 'mi': 0.06395390111152022, 'nmi': 0.05955115252882048, 'ami': 0.044321472920514915, 'npa': 0.16249999999999998, 'psi': 0.1252032520325203, 'spsi': 0.10333333333333328, 'nca': 0.17083333333333328}\n",
      "Normalized Clustering Accuracy:  0.17083333333333328\n",
      "Confusion Matrix:\n",
      " {'ar': -0.006654149922823305, 'r': 0.3780112044817927, 'fm': 0.5422516776886537, 'afm': -0.00829314089544873, 'mi': 0.025339394555509098, 'nmi': 0.03708410568112634, 'ami': 0.006735649886050927, 'npa': 0.08750000000000002, 'psi': 0.0, 'spsi': 0.0, 'nca': 0.050833333333333286}\n",
      "Normalized Clustering Accuracy:  0.050833333333333286\n",
      "Confusion Matrix:\n",
      " {'ar': 0.032432447539548485, 'r': 0.4567226890756302, 'fm': 0.5057797842506744, 'afm': 0.03644515271348557, 'mi': 0.03430842850379778, 'nmi': 0.04139592291235386, 'ami': 0.019569782187175033, 'npa': 0.13749999999999996, 'psi': 0.04194191045402122, 'spsi': 0.0, 'nca': 0.10666666666666669}\n",
      "Normalized Clustering Accuracy:  0.10666666666666669\n",
      "Confusion Matrix:\n",
      " {'ar': 0.1574966869259242, 'r': 0.5949579831932773, 'fm': 0.4851427835268327, 'afm': 0.15952327117075787, 'mi': 0.197214895822738, 'nmi': 0.19683957439246755, 'ami': 0.18262906643211896, 'npa': 0.2875000000000001, 'psi': 0.13956043956043954, 'spsi': 0.06785714285714284, 'nca': 0.32499999999999996}\n",
      "Normalized Clustering Accuracy:  0.32499999999999996\n",
      "Confusion Matrix:\n",
      " {'ar': -0.005504578186031259, 'r': 0.43515406162464987, 'fm': 0.4842089966780575, 'afm': -0.006188376114146628, 'mi': 0.007164634847128704, 'nmi': 0.008670361223061234, 'ami': -0.014143019889044152, 'npa': 0.07500000000000007, 'psi': 0.0, 'spsi': 0.0, 'nca': 0.04416666666666669}\n",
      "Normalized Clustering Accuracy:  0.04416666666666669\n",
      "Confusion Matrix:\n",
      " {'ar': -0.013181605846041416, 'r': 0.4920168067226891, 'fm': 0.4071934619985074, 'afm': -0.013609742303826625, 'mi': 0.012368483799846008, 'nmi': 0.01279518174422727, 'ami': -0.005289815878229168, 'npa': 0.07500000000000007, 'psi': 0.01066010660106599, 'spsi': 0.0, 'nca': 0.09583333333333344}\n",
      "Normalized Clustering Accuracy:  0.09583333333333344\n",
      "Confusion Matrix:\n",
      " {'ar': -0.002601084652299914, 'r': 0.48585434173669473, 'fm': 0.42821616997600864, 'afm': -0.0027214278771498603, 'mi': 0.0028433744253912874, 'nmi': 0.0030196583948627256, 'ami': -0.015851249534801833, 'npa': 0.08750000000000002, 'psi': 0.013903743315508022, 'spsi': 0.0, 'nca': 0.025833333333333375}\n",
      "Normalized Clustering Accuracy:  0.025833333333333375\n",
      "Confusion Matrix:\n",
      " {'ar': -0.015005030314368499, 'r': 0.5040616246498599, 'fm': 0.38899990553229474, 'afm': -0.015297162407959855, 'mi': 0.01598188659404498, 'nmi': 0.016074401717287426, 'ami': -0.001347856487924502, 'npa': 0.0625, 'psi': 0.0053818757419865715, 'spsi': 0.0, 'nca': 0.057499999999999996}\n",
      "Normalized Clustering Accuracy:  0.057499999999999996\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6860113896866956, 'r': 0.8719887955182073, 'fm': 0.7777535064276708, 'afm': 0.6903861523094433, 'mi': 1.0187221082553575, 'nmi': 0.7523554576331947, 'ami': 0.7417569770964927, 'npa': 0.625, 'psi': 0.2730658898713009, 'spsi': 0.24883475286701096, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.3790322580645161\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6860113896866956, 'r': 0.8719887955182073, 'fm': 0.7777535064276708, 'afm': 0.6903861523094433, 'mi': 1.0187221082553575, 'nmi': 0.7523554576331947, 'ami': 0.7417569770964927, 'npa': 0.625, 'psi': 0.2730658898713009, 'spsi': 0.24883475286701096, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.3790322580645161\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6860113896866956, 'r': 0.8719887955182073, 'fm': 0.7777535064276708, 'afm': 0.6903861523094433, 'mi': 1.0187221082553575, 'nmi': 0.7523554576331947, 'ami': 0.7417569770964927, 'npa': 0.625, 'psi': 0.2730658898713009, 'spsi': 0.24883475286701096, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.3790322580645161\n",
      "Confusion Matrix:\n",
      " {'ar': 0.649943821612011, 'r': 0.8665266106442577, 'fm': 0.7396886372637969, 'afm': 0.6499438542175191, 'mi': 1.045097635306913, 'nmi': 0.7428842200040809, 'ami': 0.7325048732443014, 'npa': 0.6875, 'psi': 0.4116403999786325, 'spsi': 0.38835116581112006, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.5489130434782609\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6860113896866956, 'r': 0.8719887955182073, 'fm': 0.7777535064276708, 'afm': 0.6903861523094433, 'mi': 1.0187221082553575, 'nmi': 0.7523554576331947, 'ami': 0.7417569770964927, 'npa': 0.625, 'psi': 0.2730658898713009, 'spsi': 0.24883475286701096, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.3790322580645161\n",
      "Confusion Matrix:\n",
      " {'ar': 0.8884037118549749, 'r': 0.9554621848739496, 'fm': 0.9209447958494184, 'afm': 0.8911016807765413, 'mi': 1.2001212346368484, 'nmi': 0.8782170082794177, 'ami': 0.873087220473533, 'npa': 0.8958333333333333, 'psi': 0.6782012313927208, 'spsi': 0.6714970903800691, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.75\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6085107451148242, 'r': 0.8323529411764706, 'fm': 0.733979918875896, 'afm': 0.6195756874823674, 'mi': 0.9367245490650762, 'nmi': 0.7087000771564204, 'ami': 0.6958240732354888, 'npa': 0.78125, 'psi': 0.5768800205225206, 'spsi': 0.5618945212493599, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.657991202346041\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6860113896866956, 'r': 0.8719887955182073, 'fm': 0.7777535064276708, 'afm': 0.6903861523094433, 'mi': 1.0187221082553575, 'nmi': 0.7523554576331947, 'ami': 0.7417569770964927, 'npa': 0.625, 'psi': 0.2730658898713009, 'spsi': 0.24883475286701096, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.3790322580645161\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6994166307700244, 'r': 0.8781512605042017, 'fm': 0.7862877356725667, 'afm': 0.7031734776768326, 'mi': 1.0306865833523242, 'nmi': 0.7595904217080426, 'ami': 0.7493327323674764, 'npa': 0.6354166666666667, 'psi': 0.28173638957049096, 'spsi': 0.2592906517445688, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.3870967741935484\n",
      "Confusion Matrix:\n",
      " {'ar': 0.799797235568715, 'r': 0.919607843137255, 'fm': 0.8573782094962484, 'afm': 0.8029011260769944, 'mi': 1.0654725798505256, 'nmi': 0.778476476890639, 'ami': 0.7691519473867133, 'npa': 0.7916666666666667, 'psi': 0.5242471813900387, 'spsi': 0.5143356643356644, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.5795454545454545\n",
      "Confusion Matrix:\n",
      " {'ar': 0.7729006525725565, 'r': 0.9086834733893557, 'fm': 0.838068266895702, 'afm': 0.7760546722614495, 'mi': 1.0024681487426865, 'nmi': 0.7334242143663753, 'ami': 0.7221835142177044, 'npa': 0.7708333333333333, 'psi': 0.49762382335299304, 'spsi': 0.4871576530061803, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.5487536656891495\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6860113896866956, 'r': 0.8719887955182073, 'fm': 0.7777535064276708, 'afm': 0.6903861523094433, 'mi': 1.0187221082553575, 'nmi': 0.7523554576331947, 'ami': 0.7417569770964927, 'npa': 0.625, 'psi': 0.2730658898713009, 'spsi': 0.24883475286701096, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.3790322580645161\n",
      "Confusion Matrix:\n",
      " {'ar': 0.0014720635130456602, 'r': 0.28165266106442577, 'fm': 0.4947609113165553, 'afm': 0.002118361234947972, 'mi': 0.03477318090311858, 'nmi': 0.04350115755948407, 'ami': -0.002639186540129862, 'npa': 0.23958333333333331, 'psi': 0.0037213337956854197, 'spsi': 0.0, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.008064516129032251\n",
      "Confusion Matrix:\n",
      " {'ar': 0.0014720635130456602, 'r': 0.28165266106442577, 'fm': 0.4947609113165553, 'afm': 0.002118361234947972, 'mi': 0.03477318090311858, 'nmi': 0.04350115755948407, 'ami': -0.002639186540129862, 'npa': 0.23958333333333331, 'psi': 0.0037213337956854197, 'spsi': 0.0, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.008064516129032251\n",
      "Confusion Matrix:\n",
      " {'ar': 0.10223917565129394, 'r': 0.6512605042016807, 'fm': 0.3389502994220394, 'afm': 0.10229214721446253, 'mi': 0.13044127278433737, 'nmi': 0.09354257943409804, 'ami': 0.05644897309194633, 'npa': 0.27083333333333337, 'psi': 0.12597435972370896, 'spsi': 0.10230283196622608, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.16088869055208466\n",
      "Confusion Matrix:\n",
      " {'ar': 0.028251844853718823, 'r': 0.35952380952380947, 'fm': 0.48063269710497303, 'afm': 0.03747635307093858, 'mi': 0.05952560653589528, 'nmi': 0.06405610958325354, 'ami': 0.005922577477257632, 'npa': 0.26041666666666663, 'psi': 0.046516664439519136, 'spsi': 0.0, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.05958498023715414\n",
      "Confusion Matrix:\n",
      " {'ar': 0.0372676329634773, 'r': 0.4753501400560224, 'fm': 0.4157864616386872, 'afm': 0.042695253315082615, 'mi': 0.07247356194095533, 'nmi': 0.06396662540224733, 'ami': 0.013459089976619957, 'npa': 0.27083333333333337, 'psi': 0.07320845053563946, 'spsi': 0.0, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.09113540736962894\n",
      "Confusion Matrix:\n",
      " {'ar': 0.13865575861497018, 'r': 0.638375350140056, 'fm': 0.39253183679188214, 'afm': 0.1404267480666107, 'mi': 0.24577454658257425, 'nmi': 0.1847526485762443, 'ami': 0.1489754335240676, 'npa': 0.28125, 'psi': 0.09362569527848641, 'spsi': 0.05963665885142966, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.191333035828127\n",
      "Confusion Matrix:\n",
      " {'ar': 0.005957031541340118, 'r': 0.4117647058823529, 'fm': 0.42434830366255494, 'afm': 0.007234772380976112, 'mi': 0.05568102123656893, 'nmi': 0.0527997076553934, 'ami': -0.002431979855912852, 'npa': 0.22916666666666669, 'psi': 0.05713798396725224, 'spsi': 0.0, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.08911449700369756\n",
      "Confusion Matrix:\n",
      " {'ar': 0.017452256278240624, 'r': 0.5582633053221289, 'fm': 0.3311747666960916, 'afm': 0.018062861092709277, 'mi': 0.05127612047213859, 'nmi': 0.03977581052009862, 'ami': -0.004198395040027001, 'npa': 0.14583333333333331, 'psi': 0.014712104937668825, 'spsi': 0.0, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.11532258064516132\n",
      "Confusion Matrix:\n",
      " {'ar': -0.010779984566256697, 'r': 0.5218487394957984, 'fm': 0.3301391515104136, 'afm': -0.011400331664929196, 'mi': 0.054184964285482584, 'nmi': 0.043359649014851766, 'ami': -0.0022710574083763006, 'npa': 0.16666666666666663, 'psi': 0.04039668217413617, 'spsi': 0.0, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.07719941348973608\n",
      "Confusion Matrix:\n",
      " {'ar': 0.014393661946369249, 'r': 0.5946778711484594, 'fm': 0.29583931759947824, 'afm': 0.014509603551396326, 'mi': 0.06876824854002472, 'nmi': 0.05071442886212101, 'ami': 0.010207308945377372, 'npa': 0.20833333333333331, 'psi': 0.07960789236810353, 'spsi': 0.04317570477434096, 'nca': nan}\n",
      "Normalized Clustering Accuracy:  0.14985337243401764\n",
      "Collection: other, Dataset: iris\n",
      "Loaded:  150  | Dimension:  4  | Label count:  1\n",
      "Generating Embeddings...\n",
      "PCA embedding done.\n",
      "t-SNE embedding done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/venv/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/venv/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP embedding done.\n",
      "MDS embedding done.\n",
      "Isomap embedding done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/venv/lib/python3.10/site-packages/sklearn/manifold/_isomap.py:384: UserWarning: The number of connected components of the neighbors graph is 2 > 1. Completing the graph to fit Isomap might be slow. Increase the number of neighbors to avoid this issue.\n",
      "  self._fit_transform(X)\n",
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/venv/lib/python3.10/site-packages/scipy/sparse/_index.py:108: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n",
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/venv/lib/python3.10/site-packages/sklearn/manifold/_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLE embedding done.\n",
      "Spectral embedding done.\n",
      "Kernel PCA embedding done.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Autoencoder embedding done.\n",
      "Random Projection embedding done.\n",
      "Truncated SVD embedding done.\n",
      "Error in numerical embeddings: The 'whiten' parameter of FastICA must be a str among {'unit-variance', 'arbitrary-variance'} or a bool among {False}. Got True instead.\n",
      "Converted numeric data to strings.\n",
      "TF-IDF (from numeric strings) embedding done.\n",
      "Count Vectorizer (from numeric strings) embedding done.\n",
      "Character-level TF-IDF (from numeric strings) embedding done.\n",
      "Hashing Vectorizer (from numeric strings) embedding done.\n",
      "SentenceTransformer (from numeric strings) embedding done.\n",
      "DistilBERT (from numeric strings) embedding done.\n",
      "Universal Sentence Encoder (from numeric strings) embedding done.\n",
      "NMF on TF-IDF (from numeric strings) embedding done.\n",
      "Doc2Vec (from numeric strings) embedding done.\n",
      "LDA on Count vector (from numeric strings) embedding done.\n",
      "Confusion Matrix:\n",
      " {'ar': 0.8857921001989628, 'r': 0.9495302013422818, 'fm': 0.9234341632720613, 'afm': 0.8857952171439673, 'mi': 0.9554359783766861, 'nmi': 0.8705214181790613, 'ami': 0.8688992655389335, 'npa': 0.94, 'psi': 0.9049707602339182, 'spsi': 0.9037037037037037, 'nca': 0.94}\n",
      "Normalized Clustering Accuracy:  0.94\n",
      "Confusion Matrix:\n",
      " {'ar': 0.703743816882739, 'r': 0.8679194630872483, 'fm': 0.803190728907463, 'afm': 0.7039393127944091, 'mi': 0.7999463111523855, 'nmi': 0.7346249737691951, 'ami': 0.7312707947305757, 'npa': 0.8200000000000001, 'psi': 0.739454094292804, 'spsi': 0.7290322580645161, 'nca': 0.8199999999999998}\n",
      "Normalized Clustering Accuracy:  0.8199999999999998\n",
      "Confusion Matrix:\n",
      " {'ar': 0.7591987071071522, 'r': 0.8922595078299776, 'fm': 0.8407289157574823, 'afm': 0.759585472485939, 'mi': 0.8744751923414558, 'nmi': 0.8056936912153364, 'ami': 0.8032287370935436, 'npa': 0.8599999999999999, 'psi': 0.7617436305732482, 'spsi': 0.7506249999999999, 'nca': 0.8599999999999999}\n",
      "Normalized Clustering Accuracy:  0.8599999999999999\n",
      "Confusion Matrix:\n",
      " {'ar': 0.745503868180448, 'r': 0.8859060402684564, 'fm': 0.8320502943378437, 'afm': 0.7460015582341927, 'mi': 0.8645238269662701, 'nmi': 0.7979885217013317, 'ami': 0.7954205025674184, 'npa': 0.8500000000000001, 'psi': 0.7472527472527473, 'spsi': 0.7346153846153847, 'nca': 0.8500000000000001}\n",
      "Normalized Clustering Accuracy:  0.8500000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.7311985567707745, 'r': 0.8797315436241611, 'fm': 0.8221697785442929, 'afm': 0.7315710577828685, 'mi': 0.8358251597124053, 'nmi': 0.7700836616487874, 'ami': 0.7671669615713114, 'npa': 0.8399999999999999, 'psi': 0.7447253184713376, 'spsi': 0.7328125000000001, 'nca': 0.8399999999999999}\n",
      "Normalized Clustering Accuracy:  0.8399999999999999\n",
      "Confusion Matrix:\n",
      " {'ar': 0.7591987071071522, 'r': 0.8922595078299776, 'fm': 0.8407289157574823, 'afm': 0.759585472485939, 'mi': 0.8744751923414558, 'nmi': 0.8056936912153364, 'ami': 0.8032287370935436, 'npa': 0.8599999999999999, 'psi': 0.7617436305732482, 'spsi': 0.7506249999999999, 'nca': 0.8599999999999999}\n",
      "Normalized Clustering Accuracy:  0.8599999999999999\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6311837368364772, 'r': 0.8322147651006712, 'fm': 0.7605570824226491, 'afm': 0.632738396199249, 'mi': 0.7519859879590751, 'nmi': 0.7039218809319661, 'ami': 0.7000972520498182, 'npa': 0.75, 'psi': 0.6336711859944716, 'spsi': 0.6080281690140845, 'nca': 0.75}\n",
      "Normalized Clustering Accuracy:  0.75\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6941455626558043, 'r': 0.8622818791946308, 'fm': 0.7991059138851095, 'afm': 0.6949008003184948, 'mi': 0.8088141560784718, 'nmi': 0.7496265610732059, 'ami': 0.746428957650967, 'npa': 0.81, 'psi': 0.7025283676255946, 'spsi': 0.6856716417910449, 'nca': 0.81}\n",
      "Normalized Clustering Accuracy:  0.81\n",
      "Confusion Matrix:\n",
      " {'ar': 0.7163421126838475, 'r': 0.8737360178970918, 'fm': 0.8112427991975698, 'afm': 0.7164832573229876, 'mi': 0.8090392795466594, 'nmi': 0.7419116631817838, 'ami': 0.738654825440286, 'npa': 0.8300000000000001, 'psi': 0.7542564967582098, 'spsi': 0.7452459016393442, 'nca': 0.8300000000000001}\n",
      "Normalized Clustering Accuracy:  0.8300000000000001\n",
      "Confusion Matrix:\n",
      " {'ar': 0.9037675791580496, 'r': 0.9574944071588367, 'fm': 0.9354538861584157, 'afm': 0.9037685868352356, 'mi': 0.9718087392798659, 'nmi': 0.8850620966553383, 'ami': 0.8836227970842695, 'npa': 0.95, 'psi': 0.9230338128152438, 'spsi': 0.9222641509433962, 'nca': 0.95}\n",
      "Normalized Clustering Accuracy:  0.95\n",
      "Confusion Matrix:\n",
      " {'ar': 0.6196749717358281, 'r': 0.8236241610738255, 'fm': 0.7591309635562491, 'afm': 0.6236410121757917, 'mi': 0.7660504764733799, 'nmi': 0.7314373897471097, 'ami': 0.7278863174680524, 'npa': 0.73, 'psi': 0.5914452519957107, 'spsi': 0.5546753246753247, 'nca': 0.73}\n",
      "Normalized Clustering Accuracy:  0.73\n",
      "Confusion Matrix:\n",
      " {'ar': 0.703743816882739, 'r': 0.8679194630872483, 'fm': 0.803190728907463, 'afm': 0.7039393127944091, 'mi': 0.7999463111523855, 'nmi': 0.7346249737691951, 'ami': 0.7312707947305757, 'npa': 0.8200000000000001, 'psi': 0.739454094292804, 'spsi': 0.7290322580645161, 'nca': 0.8199999999999998}\n",
      "Normalized Clustering Accuracy:  0.8199999999999998\n",
      "Confusion Matrix:\n",
      " {'ar': -8.890074232113897e-05, 'r': 0.3378970917225951, 'fm': 0.5657385085532538, 'afm': -0.00011978634468871814, 'mi': 0.014693108462507515, 'nmi': 0.02493178448684386, 'ami': -7.787375596613574e-05, 'npa': 0.020000000000000018, 'psi': 0.011747928833355953, 'spsi': 0.0, 'nca': 0.020000000000000018}\n",
      "Normalized Clustering Accuracy:  0.020000000000000018\n",
      "Confusion Matrix:\n",
      " {'ar': 0.000181430086369852, 'r': 0.33807606263982104, 'fm': 0.5658966686059664, 'afm': 0.00024446192793671257, 'mi': 0.014829172330138585, 'nmi': 0.02516266245476198, 'ami': 0.00015892603258110715, 'npa': 0.010000000000000009, 'psi': 0.004210240391144913, 'spsi': 0.0, 'nca': 0.010000000000000009}\n",
      "Normalized Clustering Accuracy:  0.010000000000000009\n",
      "Confusion Matrix:\n",
      " {'ar': 0.2726683658041384, 'r': 0.6663087248322148, 'fm': 0.53035012255083, 'afm': 0.2737538757605393, 'mi': 0.2815235211062286, 'nmi': 0.26661988097143663, 'ami': 0.2570072260416738, 'npa': 0.3699999999999999, 'psi': 0.2700933895561024, 'spsi': 0.2044017946161516, 'nca': 0.37}\n",
      "Normalized Clustering Accuracy:  0.37\n",
      "Confusion Matrix:\n",
      " {'ar': 0.003966000694426195, 'r': 0.5227740492170022, 'fm': 0.3824666582414667, 'afm': 0.004032852789143883, 'mi': 0.01780893727309718, 'nmi': 0.01749356611166992, 'ami': 0.004083597917485436, 'npa': 0.10000000000000009, 'psi': 0.0794471512379437, 'spsi': 0.0, 'nca': 0.09999999999999998}\n",
      "Normalized Clustering Accuracy:  0.09999999999999998\n",
      "Confusion Matrix:\n",
      " {'ar': 0.01702342913168818, 'r': 0.41413870246085005, 'fm': 0.516641344099246, 'afm': 0.020295745421295405, 'mi': 0.08633590696376559, 'nmi': 0.11170912008565492, 'ami': 0.09417401468629447, 'npa': 0.14, 'psi': 0.0855544978581628, 'spsi': 0.0, 'nca': 0.14}\n",
      "Normalized Clustering Accuracy:  0.14\n",
      "Confusion Matrix:\n",
      " {'ar': 0.13022421805568063, 'r': 0.599731543624161, 'fm': 0.4395814641690193, 'afm': 0.1308253355373665, 'mi': 0.14206804361872694, 'nmi': 0.134088106894818, 'ami': 0.12280552142920746, 'npa': 0.28, 'psi': 0.200281425891182, 'spsi': 0.12564102564102564, 'nca': 0.28}\n",
      "Normalized Clustering Accuracy:  0.28\n",
      "Confusion Matrix:\n",
      " {'ar': 0.0003038275750552155, 'r': 0.3557941834451902, 'fm': 0.5507850255204866, 'afm': 0.0003956269008434359, 'mi': 0.018903577365621815, 'nmi': 0.02921090247556147, 'ami': 0.0025814881540487432, 'npa': 0.020000000000000018, 'psi': 0.0102580372250423, 'spsi': 0.0, 'nca': 0.020000000000000018}\n",
      "Normalized Clustering Accuracy:  0.020000000000000018\n",
      "Confusion Matrix:\n",
      " {'ar': 0.0010451005909318737, 'r': 0.520089485458613, 'fm': 0.3822470014111621, 'afm': 0.0010638230001049317, 'mi': 0.01826420172810872, 'nmi': 0.017917576487409416, 'ami': 0.004556756645521768, 'npa': 0.06000000000000005, 'psi': 0.038252070510134996, 'spsi': 0.0, 'nca': 0.06000000000000005}\n",
      "Normalized Clustering Accuracy:  0.06000000000000005\n",
      "Confusion Matrix:\n",
      " {'ar': -0.0021785360929899183, 'r': 0.5480089485458612, 'fm': 0.34114336612735663, 'afm': -0.0021813038311299343, 'mi': 0.010263396251017243, 'nmi': 0.00953283623459079, 'ami': -0.003149721509849835, 'npa': 0.07999999999999996, 'psi': 0.06540593729332124, 'spsi': 0.006214979988564906, 'nca': 0.08000000000000007}\n",
      "Normalized Clustering Accuracy:  0.08000000000000007\n",
      "Confusion Matrix:\n",
      " {'ar': -0.00432164072809924, 'r': 0.5159731543624161, 'fm': 0.3807968584388768, 'afm': -0.004404651897247924, 'mi': 0.008953961509867714, 'nmi': 0.008804882227931389, 'ami': -0.004712254825517539, 'npa': 0.040000000000000036, 'psi': 0.029748283752860392, 'spsi': 0.0, 'nca': 0.040000000000000036}\n",
      "Normalized Clustering Accuracy:  0.040000000000000036\n"
     ]
    }
   ],
   "source": [
    "with open(result_csv, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for collection, datasets in eval_collections.items():\n",
    "        for dataset in datasets:\n",
    "            print(f\"Collection: {collection}, Dataset: {dataset}\")\n",
    "            X, benchmark, X_embedded_dict = load_data(collection, dataset)\n",
    "            \n",
    "            for label in range(0, len(benchmark.labels)):\n",
    "                for embedding_technique, embedded_data in X_embedded_dict.items():\n",
    "                    for clustering_method in clustering_methods:\n",
    "                        cf, nca_score = predict(\n",
    "                            embedding_technique, \n",
    "                            embedded_data, \n",
    "                            label, \n",
    "                            benchmark, \n",
    "                            clustering_method\n",
    "                        )\n",
    "                        writer.writerow([\n",
    "                            collection,          # e.g. \"wut\"\n",
    "                            dataset,             # e.g. \"x2\"\n",
    "                            clustering_method,   # e.g. \"genie\"\n",
    "                            label,               # which label set index (0, 1, ...)\n",
    "                            embedding_technique, # e.g. \"PCA\", \"t-SNE\", ...\n",
    "                            cf,                  # confusion matrix\n",
    "                            nca_score            # normalized clustering accuracy\n",
    "                        ])\n",
    "# AR (Adjusted Rand Index): Measures the similarity between two data clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings, adjusted for chance.\n",
    "\n",
    "# R (Rand Index): Similar to AR but not adjusted for chance. It measures the percentage of correct decisions made by the clustering algorithm.\n",
    "\n",
    "# FM (Fowlkes-Mallows Index): Measures the similarity between two clusterings by considering the geometric mean of the precision and recall.\n",
    "\n",
    "# AFM (Adjusted Fowlkes-Mallows Index): An adjusted version of the Fowlkes-Mallows Index that accounts for chance.\n",
    "\n",
    "# MI (Mutual Information): Measures the amount of information obtained about one clustering from the other clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collection</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Clustering Method</th>\n",
       "      <th>Label</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>NCA Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>iris</td>\n",
       "      <td>genie</td>\n",
       "      <td>0</td>\n",
       "      <td>Base</td>\n",
       "      <td>{'ar': 0.8857921001989628, 'r': 0.949530201342...</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other</td>\n",
       "      <td>iris</td>\n",
       "      <td>genie</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>{'ar': 0.9037675791580496, 'r': 0.957494407158...</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wut</td>\n",
       "      <td>x2</td>\n",
       "      <td>genie</td>\n",
       "      <td>0</td>\n",
       "      <td>Base</td>\n",
       "      <td>{'ar': 0.6882872342370341, 'r': 0.859523809523...</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wut</td>\n",
       "      <td>x2</td>\n",
       "      <td>genie</td>\n",
       "      <td>0</td>\n",
       "      <td>KernelPCA</td>\n",
       "      <td>{'ar': 0.750803955434529, 'r': 0.8880952380952...</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wut</td>\n",
       "      <td>x2</td>\n",
       "      <td>genie</td>\n",
       "      <td>1</td>\n",
       "      <td>Base</td>\n",
       "      <td>{'ar': 0.6860113896866956, 'r': 0.871988795518...</td>\n",
       "      <td>0.379032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wut</td>\n",
       "      <td>x2</td>\n",
       "      <td>genie</td>\n",
       "      <td>1</td>\n",
       "      <td>UMAP</td>\n",
       "      <td>{'ar': 0.649943821612011, 'r': 0.8665266106442...</td>\n",
       "      <td>0.548913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wut</td>\n",
       "      <td>x2</td>\n",
       "      <td>genie</td>\n",
       "      <td>1</td>\n",
       "      <td>Isomap</td>\n",
       "      <td>{'ar': 0.8884037118549749, 'r': 0.955462184873...</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wut</td>\n",
       "      <td>x2</td>\n",
       "      <td>genie</td>\n",
       "      <td>1</td>\n",
       "      <td>LLE</td>\n",
       "      <td>{'ar': 0.6085107451148242, 'r': 0.832352941176...</td>\n",
       "      <td>0.657991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wut</td>\n",
       "      <td>x2</td>\n",
       "      <td>genie</td>\n",
       "      <td>1</td>\n",
       "      <td>KernelPCA</td>\n",
       "      <td>{'ar': 0.6994166307700244, 'r': 0.878151260504...</td>\n",
       "      <td>0.387097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wut</td>\n",
       "      <td>x2</td>\n",
       "      <td>genie</td>\n",
       "      <td>1</td>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>{'ar': 0.799797235568715, 'r': 0.9196078431372...</td>\n",
       "      <td>0.579545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wut</td>\n",
       "      <td>x2</td>\n",
       "      <td>genie</td>\n",
       "      <td>1</td>\n",
       "      <td>RandomProjection</td>\n",
       "      <td>{'ar': 0.7729006525725565, 'r': 0.908683473389...</td>\n",
       "      <td>0.548754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Collection Dataset Clustering Method  Label         Embedding  \\\n",
       "0       other    iris             genie      0              Base   \n",
       "1       other    iris             genie      0       Autoencoder   \n",
       "2         wut      x2             genie      0              Base   \n",
       "3         wut      x2             genie      0         KernelPCA   \n",
       "4         wut      x2             genie      1              Base   \n",
       "5         wut      x2             genie      1              UMAP   \n",
       "6         wut      x2             genie      1            Isomap   \n",
       "7         wut      x2             genie      1               LLE   \n",
       "8         wut      x2             genie      1         KernelPCA   \n",
       "9         wut      x2             genie      1       Autoencoder   \n",
       "10        wut      x2             genie      1  RandomProjection   \n",
       "\n",
       "                                     Confusion Matrix   NCA Score  \n",
       "0   {'ar': 0.8857921001989628, 'r': 0.949530201342...    0.940000  \n",
       "1   {'ar': 0.9037675791580496, 'r': 0.957494407158...    0.950000  \n",
       "2   {'ar': 0.6882872342370341, 'r': 0.859523809523...    0.870000  \n",
       "3   {'ar': 0.750803955434529, 'r': 0.8880952380952...    0.900000  \n",
       "4   {'ar': 0.6860113896866956, 'r': 0.871988795518...    0.379032  \n",
       "5   {'ar': 0.649943821612011, 'r': 0.8665266106442...    0.548913  \n",
       "6   {'ar': 0.8884037118549749, 'r': 0.955462184873...    0.750000  \n",
       "7   {'ar': 0.6085107451148242, 'r': 0.832352941176...    0.657991  \n",
       "8   {'ar': 0.6994166307700244, 'r': 0.878151260504...    0.387097  \n",
       "9   {'ar': 0.799797235568715, 'r': 0.9196078431372...    0.579545  \n",
       "10  {'ar': 0.7729006525725565, 'r': 0.908683473389...    0.548754  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_and_compare_csv(file_path):\n",
    "    # Read the CSV file with the first line as column labels\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Initialize an empty list to store the filtered rows\n",
    "    filtered_rows = []\n",
    "    \n",
    "    # Group the DataFrame by the first four columns\n",
    "    grouped = df.groupby(['Collection', 'Dataset', 'Clustering Method', 'Label'])\n",
    "    \n",
    "    # Iterate over each group\n",
    "    for name, group in grouped:\n",
    "        # Find the \"Base\" row\n",
    "        base_row = group[group['Embedding'] == 'Base']\n",
    "        if not base_row.empty:\n",
    "            base_value = base_row.iloc[0, -1]\n",
    "            base_row_list = base_row.iloc[0].tolist()\n",
    "            base_added = False\n",
    "            \n",
    "            # Iterate over the rows in the group\n",
    "            for index, row in group.iterrows():\n",
    "                if row['Embedding'] != 'Base' and row.iloc[-1] > base_value:\n",
    "                    if not base_added:\n",
    "                        filtered_rows.append(base_row_list)\n",
    "                        base_added = True\n",
    "                    filtered_rows.append(row.tolist())\n",
    "    \n",
    "    # Create a new DataFrame from the filtered rows\n",
    "    filtered_df = pd.DataFrame(filtered_rows, columns=df.columns)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    filtered_df = filtered_df.drop_duplicates()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Example usage\n",
    "file_path = '/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/framework/results/v1_test.csv'\n",
    "filtered_df = filter_and_compare_csv(file_path)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
