{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clustbench \n",
    "import os.path, genieclust, sklearn.cluster # we will need these later\n",
    "import matplotlib.pyplot as plt, numpy as np, pandas as pd\n",
    "import csv\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift, SpectralClustering, AffinityPropagation, OPTICS, Birch, MiniBatchKMeans, SpectralCoclustering\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import hdbscan\n",
    "from kmodes.kmodes import KModes\n",
    "from fcmeans import FCM\n",
    "from minisom import MiniSom\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import pairwise, accuracy_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.cluster import rand_score, adjusted_rand_score, fowlkes_mallows_score, mutual_info_score, adjusted_mutual_info_score, normalized_mutual_info_score\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(X):\n",
    "    \"\"\"\n",
    "    Generate a dictionary of embeddings from a multidimensional NumPy array of numbers.\n",
    "\n",
    "    This function applies both traditional numerical embeddings and text-based embedding\n",
    "    techniques (by converting numeric rows to strings) to produce a variety of representations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        A 2D array of shape (n_samples, n_features) containing numeric data.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary where the key \"Base\" corresponds to the original data and additional\n",
    "        keys correspond to various embedded representations.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    embeddings = {\"Base\": X}\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1. Traditional Numerical Embeddings\n",
    "    # ------------------------------------------------\n",
    "    try:\n",
    "        from sklearn.decomposition import (\n",
    "            PCA, KernelPCA, TruncatedSVD, FastICA, FactorAnalysis\n",
    "        )\n",
    "        from sklearn.manifold import (\n",
    "            TSNE, MDS, Isomap, LocallyLinearEmbedding, SpectralEmbedding\n",
    "        )\n",
    "        from sklearn.random_projection import GaussianRandomProjection\n",
    "        \n",
    "        # PCA\n",
    "        # Common default: n_components=2 for visualization; whiten=False (unless needed)\n",
    "        pca_model = PCA(n_components=2, whiten=False, random_state=42)\n",
    "        embeddings[\"PCA\"] = pca_model.fit_transform(X)\n",
    "        print(\"PCA embedding done.\")\n",
    "\n",
    "        # t-SNE\n",
    "        # Common defaults: n_components=2, perplexity=30, learning_rate='auto', n_iter=1000+\n",
    "        # Note: t-SNE can be slow on large datasets. \n",
    "        tsne_model = TSNE(\n",
    "            n_components=2,\n",
    "            perplexity=30,\n",
    "            learning_rate='auto',\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            init='pca'  # often helps with convergence\n",
    "        )\n",
    "        embeddings[\"t-SNE\"] = tsne_model.fit_transform(X)\n",
    "        print(\"t-SNE embedding done.\")\n",
    "\n",
    "        # UMAP (requires umap-learn)\n",
    "        # Common defaults: n_neighbors=15, min_dist=0.1, metric='euclidean'\n",
    "        # NOTE: This can be slow for large data. Increase n_epochs or do PCA first if needed.\n",
    "        try:\n",
    "            import umap\n",
    "            umap_model = umap.UMAP(\n",
    "                n_components=2,\n",
    "                n_neighbors=15,\n",
    "                min_dist=0.1,\n",
    "                metric='euclidean',\n",
    "                random_state=42\n",
    "            )\n",
    "            embeddings[\"UMAP\"] = umap_model.fit_transform(X)\n",
    "            print(\"UMAP embedding done.\")\n",
    "        except Exception as e:\n",
    "            print(\"UMAP embedding failed:\", e)\n",
    "\n",
    "        # MDS\n",
    "        # Default: n_components=2, metric=True (classical MDS), can be slow for large data\n",
    "        mds_model = MDS(n_components=2, metric=True, random_state=42, n_init=4, max_iter=300)\n",
    "        embeddings[\"MDS\"] = mds_model.fit_transform(X)\n",
    "        print(\"MDS embedding done.\")\n",
    "\n",
    "        # Isomap\n",
    "        # Default: n_neighbors=5, n_components=2\n",
    "        isomap_model = Isomap(n_components=2, n_neighbors=5)\n",
    "        embeddings[\"Isomap\"] = isomap_model.fit_transform(X)\n",
    "        print(\"Isomap embedding done.\")\n",
    "\n",
    "        # Locally Linear Embedding (LLE)\n",
    "        # Default: n_neighbors=10, n_components=2\n",
    "        lle_model = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "        embeddings[\"LLE\"] = lle_model.fit_transform(X)\n",
    "        print(\"LLE embedding done.\")\n",
    "\n",
    "        # Spectral Embedding\n",
    "        # Default: n_components=2, affinity='nearest_neighbors', n_neighbors=5\n",
    "        spectral_model = SpectralEmbedding(\n",
    "            n_components=2,\n",
    "            n_neighbors=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        embeddings[\"Spectral\"] = spectral_model.fit_transform(X)\n",
    "        print(\"Spectral embedding done.\")\n",
    "\n",
    "        # Kernel PCA with RBF kernel\n",
    "        # Common defaults: n_components=2, kernel='rbf', gamma=None (auto)\n",
    "        kpca_model = KernelPCA(\n",
    "            n_components=2,\n",
    "            kernel='rbf',\n",
    "            gamma=None,\n",
    "            random_state=42\n",
    "        )\n",
    "        embeddings[\"KernelPCA\"] = kpca_model.fit_transform(X)\n",
    "        print(\"Kernel PCA embedding done.\")\n",
    "\n",
    "        # Autoencoder embedding (using TensorFlow/Keras)\n",
    "        # Basic architecture: input->(64)->(32)->(2)->(32)->(64)->output\n",
    "        # Epochs, batch_size can be tuned for better performance\n",
    "        try:\n",
    "            import tensorflow as tf\n",
    "            from tensorflow.keras.layers import Input, Dense\n",
    "            from tensorflow.keras.models import Model\n",
    "\n",
    "            input_dim = X.shape[1]\n",
    "            encoding_dim = 2  # target dimension\n",
    "            input_layer = Input(shape=(input_dim,))\n",
    "            encoded = Dense(64, activation='relu')(input_layer)\n",
    "            encoded = Dense(32, activation='relu')(encoded)\n",
    "            bottleneck = Dense(encoding_dim, activation='linear')(encoded)\n",
    "            decoded = Dense(32, activation='relu')(bottleneck)\n",
    "            decoded = Dense(64, activation='relu')(decoded)\n",
    "            output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "            autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "            autoencoder.compile(optimizer='adam', loss='mse')\n",
    "            # Train briefly (increase epochs for better results on real data)\n",
    "            autoencoder.fit(X, X, epochs=50, batch_size=32, verbose=0)\n",
    "            encoder = Model(inputs=input_layer, outputs=bottleneck)\n",
    "            embeddings[\"Autoencoder\"] = encoder.predict(X)\n",
    "            print(\"Autoencoder embedding done.\")\n",
    "        except Exception as e:\n",
    "            print(\"Autoencoder embedding failed:\", e)\n",
    "\n",
    "        # Random Projection\n",
    "        # Common default: n_components=2, use GaussianRandomProjection\n",
    "        rp = GaussianRandomProjection(n_components=2, eps=0.1, random_state=42)\n",
    "        embeddings[\"RandomProjection\"] = rp.fit_transform(X)\n",
    "        print(\"Random Projection embedding done.\")\n",
    "\n",
    "        # Truncated SVD\n",
    "        # Common default: n_components=2, good for sparse data (like TF-IDF)\n",
    "        svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "        embeddings[\"TruncatedSVD\"] = svd.fit_transform(X)\n",
    "        print(\"Truncated SVD embedding done.\")\n",
    "\n",
    "        # FastICA\n",
    "        # Common defaults: n_components=2, whiten=True, max_iter=200\n",
    "        ica = FastICA(n_components=2, whiten=True, max_iter=200, random_state=42)\n",
    "        embeddings[\"FastICA\"] = ica.fit_transform(X)\n",
    "        print(\"FastICA embedding done.\")\n",
    "\n",
    "        # Factor Analysis\n",
    "        # Default: n_components=2\n",
    "        fa = FactorAnalysis(n_components=2, random_state=42)\n",
    "        embeddings[\"FactorAnalysis\"] = fa.fit_transform(X)\n",
    "        print(\"Factor Analysis embedding done.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error in numerical embeddings:\", e)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. Text-Based Embeddings on Numeric Data\n",
    "    #    (Convert each row to a string, then treat it as text)\n",
    "    # ------------------------------------------------\n",
    "    try:\n",
    "        # Convert each row (sample) to a space-separated string, e.g., \"0.12 0.57 0.99 ...\"\n",
    "        X_as_str = [\" \".join(map(str, row)) for row in X]\n",
    "        print(\"Converted numeric data to strings.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error converting numeric data to strings:\", e)\n",
    "        X_as_str = []\n",
    "\n",
    "    # TF-IDF Vectorizer\n",
    "    # Common defaults: max_features=500 (can raise if large vocabulary)\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
    "        X_tfidf = tfidf_vectorizer.fit_transform(X_as_str).toarray()\n",
    "        embeddings[\"TF-IDF_str\"] = X_tfidf\n",
    "        print(\"TF-IDF (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"TF-IDF embedding error:\", e)\n",
    "\n",
    "    # Count Vectorizer\n",
    "    # Common defaults: max_features=500\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        count_vectorizer = CountVectorizer(max_features=500)\n",
    "        X_count = count_vectorizer.fit_transform(X_as_str).toarray()\n",
    "        embeddings[\"Count_str\"] = X_count\n",
    "        print(\"Count Vectorizer (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Count Vectorizer embedding error:\", e)\n",
    "\n",
    "    # Character-level TF-IDF\n",
    "    # Common defaults: analyzer='char', ngram_range=(2,4), max_features=500\n",
    "    try:\n",
    "        char_vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(2, 4),\n",
    "            max_features=500\n",
    "        )\n",
    "        X_char_tfidf = char_vectorizer.fit_transform(X_as_str).toarray()\n",
    "        embeddings[\"CharTFIDF_str\"] = X_char_tfidf\n",
    "        print(\"Character-level TF-IDF (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Character-level TF-IDF error:\", e)\n",
    "\n",
    "    # Hashing Vectorizer\n",
    "    # Common defaults: n_features=500\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import HashingVectorizer\n",
    "        hv = HashingVectorizer(n_features=500)\n",
    "        X_hash = hv.transform(X_as_str).toarray()\n",
    "        embeddings[\"Hashing_str\"] = X_hash\n",
    "        print(\"Hashing Vectorizer (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Hashing Vectorizer error:\", e)\n",
    "\n",
    "    # SentenceTransformer embedding (requires sentence-transformers)\n",
    "    # Example model: paraphrase-MiniLM-L6-v2 (small & fast)\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        st_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        X_st = st_model.encode(X_as_str, show_progress_bar=False)\n",
    "        embeddings[\"SentenceTransformer_str\"] = X_st\n",
    "        print(\"SentenceTransformer (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"SentenceTransformer embedding error:\", e)\n",
    "\n",
    "    # DistilBERT embedding via HuggingFace Transformers\n",
    "    # For each row-as-string, tokenize and average the hidden states\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        import torch\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        def get_distilbert_embedding(text):\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            # Mean pooling over token embeddings\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()[0]\n",
    "            return embedding\n",
    "\n",
    "        X_distilbert = np.array([get_distilbert_embedding(txt) for txt in X_as_str])\n",
    "        embeddings[\"DistilBERT_str\"] = X_distilbert\n",
    "        print(\"DistilBERT (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"DistilBERT embedding error:\", e)\n",
    "\n",
    "    # Universal Sentence Encoder (USE) via TensorFlow Hub\n",
    "    # Good universal text embedding, some limitations on max length \n",
    "    try:\n",
    "        import tensorflow_hub as hub\n",
    "        use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        X_use = use_model(X_as_str).numpy()\n",
    "        embeddings[\"USE_str\"] = X_use\n",
    "        print(\"Universal Sentence Encoder (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Universal Sentence Encoder embedding error:\", e)\n",
    "\n",
    "    # NMF on the TF-IDF representation (to further reduce dimensionality)\n",
    "    # Common defaults: n_components=2, init='nndsvd'\n",
    "    try:\n",
    "        from sklearn.decomposition import NMF\n",
    "        nmf_model = NMF(n_components=2, init='nndsvd', random_state=42, max_iter=200)\n",
    "        X_nmf = nmf_model.fit_transform(X_tfidf)\n",
    "        embeddings[\"NMF_TFIDF_str\"] = X_nmf\n",
    "        print(\"NMF on TF-IDF (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"NMF embedding error:\", e)\n",
    "\n",
    "    # Doc2Vec embedding using gensim\n",
    "    # vector_size=50, min_count=1, epochs=40 are typical defaults for small data\n",
    "    try:\n",
    "        from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "        documents = [TaggedDocument(words=txt.split(), tags=[str(i)]) for i, txt in enumerate(X_as_str)]\n",
    "        doc2vec_model = Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
    "        doc2vec_model.build_vocab(documents)\n",
    "        doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "        X_doc2vec = np.array([doc2vec_model.infer_vector(txt.split()) for txt in X_as_str])\n",
    "        embeddings[\"Doc2Vec_str\"] = X_doc2vec\n",
    "        print(\"Doc2Vec (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Doc2Vec embedding error:\", e)\n",
    "\n",
    "    # Optional: Add Latent Dirichlet Allocation (LDA) for topic modeling on the string data\n",
    "    # This can be done on the TF-IDF or Count vector for text\n",
    "    try:\n",
    "        from sklearn.decomposition import LatentDirichletAllocation\n",
    "        # For demonstration, let's do LDA with 5 topics on the Count vector\n",
    "        if \"Count_str\" in embeddings:\n",
    "            lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "            X_lda = lda_model.fit_transform(embeddings[\"Count_str\"])\n",
    "            embeddings[\"LDA_Count_str\"] = X_lda\n",
    "            print(\"LDA on Count vector (from numeric strings) embedding done.\")\n",
    "    except Exception as e:\n",
    "        print(\"LDA embedding error:\", e)\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/gagolews/clustering-data-v1/raw/v1.1.0\"\n",
    "\n",
    "def load_data(collection, dataset):\n",
    "    benchmark = clustbench.load_dataset(collection, dataset, url=data_url)\n",
    "    X = benchmark.data\n",
    "    print(\"Loaded: \", X.shape[0], \" | Dimension: \", X.shape[1], \" | Label count: \", len(benchmark.labels))\n",
    "    print(\"Generating Embeddings...\")\n",
    "    return X, benchmark, generate_embeddings(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "each dataset can have multiple labels, \n",
    "pick one at a time and that defines your partition size, aka k\n",
    "\n",
    "Overall, Genie returned a clustering quite similar to the reference one. We may consider 107\n",
    "(namely, c11 + c22 + c33 ) out of the 120 input points as correctly grouped. In particular, \n",
    "all the red and green reference points (the 2nd and the 3rd row) have been properly discovered.\n",
    "\n",
    "Normalized Clustering Accuracy (NCA) \n",
    "NCA is the averaged percentage of correctly classified points in each cluster \n",
    "above the perfectly uniform label distribution.\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def predict(embedding_technique, X, label, benchmark, clustering_method, plot=False):\n",
    "\n",
    "    y_true = benchmark.labels[label] \n",
    "    (k := max(y_true))  # or benchmark.n_clusters[0]\n",
    "    m = max(min(y_true),2)\n",
    "    method = clustering_method.lower()\n",
    "\n",
    "    # Define the clustering model\n",
    "    if method == \"genie\":\n",
    "        model = genieclust.Genie(n_clusters=k)  # using default parameters\n",
    "    elif method == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    elif method == \"agglomerative\":\n",
    "        model = AgglomerativeClustering(n_clusters=k)\n",
    "    elif method == \"dbscan\":\n",
    "        model = DBSCAN(eps=0.2, min_samples=m)\n",
    "    elif method == \"meanshift\":\n",
    "        model = MeanShift()\n",
    "    elif method == \"spectral\":\n",
    "        model = SpectralClustering(n_clusters=k, random_state=42)\n",
    "    elif method == \"affinitypropagation\":\n",
    "        model =  AffinityPropagation(random_state=42, convergence_iter= 5, max_iter= 100)\n",
    "    elif method == \"optics\":\n",
    "        model = OPTICS()\n",
    "    elif method == \"gaussianmixture\":\n",
    "        model = GaussianMixture(n_components=k, random_state=42)\n",
    "    elif method == \"hdbscan\":\n",
    "        model = hdbscan.HDBSCAN(min_cluster_size=m)\n",
    "    elif method == \"kmodes\":\n",
    "        model = KModes(n_clusters=k, random_state=42, init=\"Huang\")\n",
    "    elif method == \"birch\":\n",
    "        model = Birch(n_clusters=k)\n",
    "    elif method == \"minibatchkmeans\":\n",
    "        model = MiniBatchKMeans(n_clusters=k, random_state=42)\n",
    "    elif method == \"kmedoids\":\n",
    "        model = KMedoids(n_clusters=k, random_state=42)\n",
    "    elif method == \"latentdirichletallocation\":\n",
    "        X = np.maximum(X, 0)\n",
    "        model = LatentDirichletAllocation(n_components=k, random_state=42)\n",
    "    elif method == \"spectralcoclustering\":\n",
    "        model =  SpectralCoclustering(n_clusters=k)\n",
    "    elif method == \"bayesiangaussianmixture\":\n",
    "        model = BayesianGaussianMixture(n_components=k)   \n",
    "\n",
    "    print(\"The model: \" +  method + \" has been trained now getting y_pred\") \n",
    "   \n",
    "    # Fit the model and predict the cluster labels\n",
    "    if method == \"gaussianmixture\":  # Gaussian uses predict instead of fit_predict\n",
    "        (y_pred := model.fit(X).predict(X) + 1)\n",
    "    elif method == \"latentdirichletallocation\":\n",
    "        model.fit(X)\n",
    "        y_pred = model.transform(X).argmax(axis=1) + 1\n",
    "    elif method == \"spectralcoclustering\":\n",
    "        model.fit(X)\n",
    "        y_pred = y_pred = model.row_labels_ + 1\n",
    "    elif method == \"optics\" or method == \"hdbscan\" or method == \"dbscan\":\n",
    "        y_pred = model.fit_predict(X)\n",
    "        unique_labels = np.unique(y_pred)\n",
    "        if -1 in unique_labels:\n",
    "            y_pred = np.where(y_pred == -1, max(unique_labels) + 1, y_pred)  # Assign noise to a new cluster\n",
    "        y_pred += 1\n",
    "    else:\n",
    "        (y_pred := model.fit_predict(X) + 1)\n",
    "        \n",
    "    assert len(y_true) == len(y_pred), \"Length of y_true: \" + str(len(y_true)) +  \"and y_pred: \" + str(len(y_pred)) + \"are not the same\"\n",
    "    print(\"y_true dimensions: \" + str(y_true.shape))\n",
    "    print(\"y_pred dimensions: \" + str(y_pred.shape))\n",
    "    print(\"max(y_true): \" + str(max(y_true)))\n",
    "    print(\"max(y_pred): \" + str(max(y_pred)))\n",
    "    print(\"min(y_true): \" + str(min(y_true)))\n",
    "    print(\"min(y_pred): \" + str(min(y_pred)))\n",
    "\n",
    "    nca = clustbench.get_score(y_true, y_pred)\n",
    "    cf = metrics.confusion_matrix(y_true, y_pred)\n",
    "    nca = clustbench.get_score(y_true, y_pred)\n",
    "    r = rand_score(y_true, y_pred)\n",
    "    ar = adjusted_rand_score(y_true, y_pred)\n",
    "    fm = fowlkes_mallows_score(y_true, y_pred)\n",
    "    mi = mutual_info_score(y_true, y_pred)\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    ami = adjusted_mutual_info_score(y_true, y_pred)\n",
    "    a = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "\n",
    "\n",
    "    if plot:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        model.plots.plot_scatter(X, labels=y_true-1, axis=\"equal\", title=\"y_true\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        model.plots.plot_scatter(X, labels=y_pred-1, axis=\"equal\", title=\"y_pred\")\n",
    "        plt.show()\n",
    "\n",
    "    return cf, nca, r, ar, fm, mi, nmi, ami, a\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd()) # run to check current working directory and update file path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_collections = {\"wut\": [\"x2\"], \"other\": [\"iris\"]}\n",
    "clustering_methods = [\"genie\", \"kmeans\", \"agglomerative\", \"dbscan\", \"meanshift\", \"spectral\", \"affinitypropagation\",\"optics\",\"gaussianmixture\", \"hdbscan\", \"kmodes\", \"birch\", \"minibatchkmeans\" ,\"kmedoids\", \"latentdirichletallocation\", \"spectralcoclustering\", \"bayesiangaussianmixture\"]\n",
    "result_csv = \"/Users/ikshitayadav/dev/embedding_based_clustering_research/framework/results/v1_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Run to set the column names for the csv file\n",
    "\"\"\"\n",
    "import os\n",
    "import csv\n",
    "\n",
    "if os.path.exists(result_csv):\n",
    "    print(\"File already exists\")\n",
    "else:\n",
    "    try:\n",
    "        with open(result_csv, mode='w', newline='') as file: \n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Collection\", \"Dataset\", \"Clustering Method\", \"Label\", \"Embedding\", \"CF\", \"NCA Score\", \"R\", \"AR\", \"FM\", \"MI\", \"NMI\", \"AMI\", \"A\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error writing to file: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe create a cache or temporary storage for the embeddings\n",
    "# TODO: parallelize the embedding and clustering process per dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_csv, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for collection, datasets in eval_collections.items():\n",
    "        for dataset in datasets:\n",
    "            print(f\"Collection: {collection}, Dataset: {dataset}\")\n",
    "            X, benchmark, X_embedded_dict = load_data(collection, dataset)\n",
    "            \n",
    "            for label in range(0, len(benchmark.labels)):\n",
    "                for embedding_technique, embedded_data in X_embedded_dict.items():\n",
    "                    for clustering_method in clustering_methods:\n",
    "                        cf, nca_score, r, ar, fm, mi, nmi, ami, a = predict(\n",
    "                            embedding_technique, \n",
    "                            embedded_data, \n",
    "                            label, \n",
    "                            benchmark, \n",
    "                            clustering_method\n",
    "                        )\n",
    "\n",
    "                        writer.writerow([\n",
    "                            collection,          # e.g. \"wut\"\n",
    "                            dataset,             # e.g. \"x2\"\n",
    "                            clustering_method,   # e.g. \"genie\"\n",
    "                            label,               # which label set index (0, 1, ...)\n",
    "                            embedding_technique, # e.g. \"PCA\", \"t-SNE\", ...\n",
    "                            f\"cf: {cf} \", # confusion matrix\n",
    "                            f\"nca: {nca_score} \", # normalized clustering accuracy\n",
    "                            f\"r: {r} \",  # rand index\n",
    "                            f\"ar: {ar} \",  # adjusted rand index\n",
    "                            f\"fm: {fm} \",  # fowlkes-mallows index\n",
    "                            f\"mi: {mi} \",  # mutual information\n",
    "                            f\"nmi: {nmi} \",  # normalized mutual information\n",
    "                            f\"ami: {ami} \",  # adjusted mutual information\n",
    "                            f\"a: {a} \",  # accuracy score\n",
    "                        ])\n",
    "# AR (Adjusted Rand Index): Measures the similarity between two data clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings, adjusted for chance.\n",
    "\n",
    "# R (Rand Index): Similar to AR but not adjusted for chance. It measures the percentage of correct decisions made by the clustering algorithm.\n",
    "\n",
    "# FM (Fowlkes-Mallows Index): Measures the similarity between two clusterings by considering the geometric mean of the precision and recall.\n",
    "\n",
    "# AFM (Adjusted Fowlkes-Mallows Index): An adjusted version of the Fowlkes-Mallows Index that accounts for chance.\n",
    "\n",
    "# MI (Mutual Information): Measures the amount of information obtained about one clustering from the other clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_and_compare_csv(file_path):\n",
    "    # Read the CSV file with the first line as column labels\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Initialize an empty list to store the filtered rows\n",
    "    filtered_rows = []\n",
    "    \n",
    "    # Group the DataFrame by the first four columns\n",
    "    grouped = df.groupby(['Collection', 'Dataset', 'Clustering Method', 'Label'])\n",
    "    \n",
    "    # Iterate over each group\n",
    "    for name, group in grouped:\n",
    "        # Find the \"Base\" row\n",
    "        base_row = group[group['Embedding'] == 'Base']\n",
    "        if not base_row.empty:\n",
    "            base_value = base_row.iloc[0, -1]\n",
    "            base_row_list = base_row.iloc[0].tolist()\n",
    "            base_added = False\n",
    "            \n",
    "            # Iterate over the rows in the group\n",
    "            for index, row in group.iterrows():\n",
    "                if row['Embedding'] != 'Base' and row.iloc[-1] > base_value:\n",
    "                    if not base_added:\n",
    "                        filtered_rows.append(base_row_list)\n",
    "                        base_added = True\n",
    "                    filtered_rows.append(row.tolist())\n",
    "    \n",
    "    # Create a new DataFrame from the filtered rows\n",
    "    filtered_df = pd.DataFrame(filtered_rows, columns=df.columns)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    filtered_df = filtered_df.drop_duplicates()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Example usage\n",
    "file_path = '/Users/ikshitayadav/dev/embedding_based_clustering_research/framework/results/v1_test.csv'\n",
    "filtered_df = filter_and_compare_csv(file_path)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
