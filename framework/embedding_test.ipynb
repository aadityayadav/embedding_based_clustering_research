{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 04:39:32.444778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import clustbench\n",
    "import genieclust\n",
    "import sklearn.cluster\n",
    "from sklearn.cluster import (\n",
    "    KMeans, AgglomerativeClustering, DBSCAN, MeanShift, SpectralClustering,\n",
    "    AffinityPropagation, OPTICS, Birch, MiniBatchKMeans, SpectralCoclustering\n",
    ")\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.decomposition import (\n",
    "    PCA, KernelPCA, TruncatedSVD, FastICA, FactorAnalysis, LatentDirichletAllocation, NMF\n",
    ")\n",
    "from sklearn.manifold import (\n",
    "    TSNE, MDS, Isomap, LocallyLinearEmbedding, SpectralEmbedding\n",
    ")\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import (\n",
    "    TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    pairwise, accuracy_score, silhouette_score, rand_score, adjusted_rand_score,\n",
    "    fowlkes_mallows_score, mutual_info_score, adjusted_mutual_info_score, normalized_mutual_info_score\n",
    ")\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import hdbscan\n",
    "from kmodes.kmodes import KModes\n",
    "from fcmeans import FCM\n",
    "from minisom import MiniSom\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn import metrics\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertModel, AutoTokenizer, AutoModel\n",
    ")\n",
    "import tensorflow_hub as hub\n",
    "import umap\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    \"\"\"Convert each row of X into a string, joined by spaces.\"\"\"\n",
    "    return [\" \".join(map(str, row)) for row in X]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Text-based Embedding Functions\n",
    "# -------------------------------------------------------------------\n",
    "def generate_TFIDF_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    vectorizer = TfidfVectorizer(max_features=500)\n",
    "    return vectorizer.fit_transform(X_as_str).toarray()\n",
    "\n",
    "def generate_CountVectorizer_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    vectorizer = CountVectorizer(max_features=500)\n",
    "    return vectorizer.fit_transform(X_as_str).toarray()\n",
    "\n",
    "def generate_CharTFIDF_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=500)\n",
    "    return vectorizer.fit_transform(X_as_str).toarray()\n",
    "\n",
    "def generate_Hashing_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    vectorizer = HashingVectorizer(n_features=500)\n",
    "    return vectorizer.transform(X_as_str).toarray()\n",
    "\n",
    "def generate_SentenceTransformer_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    st_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    return st_model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "def generate_DistilBERT_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def get_distilbert_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Mean pooling over token embeddings\n",
    "        return outputs.last_hidden_state.mean(dim=1).detach().numpy()[0]\n",
    "\n",
    "    return np.array([get_distilbert_embedding(txt) for txt in X_as_str])\n",
    "\n",
    "def generate_Doc2Vec_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    documents = [TaggedDocument(words=txt.split(), tags=[str(i)]) for i, txt in enumerate(X_as_str)]\n",
    "    doc2vec_model = Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
    "    doc2vec_model.build_vocab(documents)\n",
    "    doc2vec_model.train(documents, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "    return np.array([doc2vec_model.infer_vector(txt.split()) for txt in X_as_str])\n",
    "\n",
    "def generate_multilingual_e5_large_instruct_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    model = SentenceTransformer('intfloat/multilingual-e5-large-instruct')\n",
    "    return model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "def generate_KaLM_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    model = SentenceTransformer('HIT-TMG/KaLM-embedding-multilingual-mini-v1')\n",
    "    return model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "\n",
    "def generate_mxbai_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    model = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1')\n",
    "    return model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "def generate_bge_embedding(X, X_as_str=None):\n",
    "    if X_as_str is None:\n",
    "        X_as_str = preprocess_data(X)\n",
    "    model = SentenceTransformer('BAAI/bge-reranker-large')\n",
    "    return model.encode(X_as_str, show_progress_bar=False)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Non-text Embedding Functions (unchanged)\n",
    "# -------------------------------------------------------------------\n",
    "def generate_PCA_embedding(X):\n",
    "    return PCA(n_components=2, whiten=False, random_state=42).fit_transform(X)\n",
    "\n",
    "def generate_TSNE_embedding(X):\n",
    "    return TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=30,\n",
    "        learning_rate='auto',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        init='pca'\n",
    "    ).fit_transform(X)\n",
    "\n",
    "def generate_UMAP_embedding(X):\n",
    "    return umap.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=15,\n",
    "        min_dist=0.1,\n",
    "        metric='euclidean',\n",
    "        random_state=42\n",
    "    ).fit_transform(X)\n",
    "\n",
    "def generate_MDS_embedding(X):\n",
    "    return MDS(n_components=2, metric=True, random_state=42, n_init=4, max_iter=300).fit_transform(X)\n",
    "\n",
    "def generate_Isomap_embedding(X):\n",
    "    return Isomap(n_components=2, n_neighbors=5).fit_transform(X)\n",
    "\n",
    "def generate_LLE_embedding(X):\n",
    "    return LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42).fit_transform(X)\n",
    "\n",
    "def generate_SpectralEmbedding_embedding(X):\n",
    "    return SpectralEmbedding(\n",
    "        n_components=2,\n",
    "        n_neighbors=5,\n",
    "        random_state=42\n",
    "    ).fit_transform(X)\n",
    "\n",
    "def generate_KernelPCA_embedding(X):\n",
    "    return KernelPCA(\n",
    "        n_components=2,\n",
    "        kernel='rbf',\n",
    "        gamma=None,\n",
    "        random_state=42\n",
    "    ).fit_transform(X)\n",
    "\n",
    "def generate_Gaussian_random_projection(X):\n",
    "    return GaussianRandomProjection(n_components=2, eps=0.1, random_state=42).fit_transform(X)\n",
    "\n",
    "def generate_TruncatedSVD_embedding(X):\n",
    "    return TruncatedSVD(n_components=2, random_state=42).fit_transform(X)\n",
    "\n",
    "def generate_FastICA_embedding(X):\n",
    "    return FastICA(n_components=2, whiten=True, max_iter=200, random_state=42).fit_transform(X)\n",
    "\n",
    "def generate_FactorAnalysis_embedding(X):\n",
    "    return FactorAnalysis(n_components=2, random_state=42).fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(X):\n",
    "    # Precompute the text representation once\n",
    "    X_as_str = preprocess_data(X)\n",
    "    \n",
    "    # List your embedding functions as tuples: (name, function, is_text_based)\n",
    "    embedding_functions = [\n",
    "        (\"PCA\", generate_PCA_embedding, False),\n",
    "        (\"KernelPCA\", generate_KernelPCA_embedding, False),\n",
    "        (\"TruncatedSVD\", generate_TruncatedSVD_embedding, False),\n",
    "        (\"FactorAnalysis\", generate_FactorAnalysis_embedding, False),\n",
    "        (\"TSNE\", generate_TSNE_embedding, False),\n",
    "        (\"UMAP\", generate_UMAP_embedding, False),\n",
    "        (\"MDS\", generate_MDS_embedding, False),\n",
    "        (\"Isomap\", generate_Isomap_embedding, False),\n",
    "        (\"LLE\", generate_LLE_embedding, False),\n",
    "        (\"SpectralEmbedding\", generate_SpectralEmbedding_embedding, False),\n",
    "        (\"GaussianRP\", generate_Gaussian_random_projection, False),\n",
    "        # (\"TFIDF\", generate_TFIDF_embedding, True),\n",
    "        # (\"CountVectorizer\", generate_CountVectorizer_embedding, True),\n",
    "        # (\"CharTFIDF\", generate_CharTFIDF_embedding, True),\n",
    "        # (\"Hashing\", generate_Hashing_embedding, True),\n",
    "        # (\"SentenceTransformer\", generate_SentenceTransformer_embedding, True),\n",
    "        # (\"DistilBERT\", generate_DistilBERT_embedding, True),\n",
    "        # (\"Doc2Vec\", generate_Doc2Vec_embedding, True),\n",
    "        # (\"multilingual_e5_large_instruct\", generate_multilingual_e5_large_instruct_embedding, True),\n",
    "        # (\"KaLM\", generate_KaLM_embedding, True),\n",
    "        # (\"mxbai\", generate_mxbai_embedding, True),\n",
    "        # (\"bge\", generate_bge_embedding, True),\n",
    "    ]\n",
    "    \n",
    "    results_dict = {}\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {}\n",
    "        for name, func, is_text in embedding_functions:\n",
    "            # For text-based functions, pass the precomputed X_as_str; otherwise, just pass X.\n",
    "            if is_text:\n",
    "                futures[executor.submit(func, X, X_as_str)] = name\n",
    "            else:\n",
    "                futures[executor.submit(func, X)] = name\n",
    "\n",
    "        for future in futures:\n",
    "            func_name = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {func_name}: {e}\")\n",
    "                result = None\n",
    "            results_dict[func_name] = result\n",
    "\n",
    "    results_dict['Base'] = X\n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/gagolews/clustering-data-v1/raw/v1.1.0\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "def get_cached_embeddings(collection, dataset, X):\n",
    "    cache_dir = \"embedding_cache\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_file = os.path.join(cache_dir, f\"{collection}_{dataset}_embeddings.pkl\")\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(\"Loading embeddings from cache:\", cache_file)\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            embeddings = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Cache not found. Generating embeddings...\")\n",
    "        embeddings = generate_embeddings(X)\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "        print(\"Embeddings cached at:\", cache_file)\n",
    "    return embeddings\n",
    "\n",
    "def load_data(collection, dataset):\n",
    "    benchmark = clustbench.load_dataset(collection, dataset, url=data_url)\n",
    "    X = benchmark.data\n",
    "    print(\"Loaded: \", X.shape[0], \" | Dimension: \", X.shape[1], \" | Label count: \", len(benchmark.labels))\n",
    "    print(\"Getting embeddings (with caching)...\")\n",
    "    X_embedded_dict = get_cached_embeddings(collection, dataset, X)\n",
    "    return X, benchmark, X_embedded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "each dataset can have multiple labels, \n",
    "pick one at a time and that defines your partition size, aka k\n",
    "\n",
    "Overall, Genie returned a clustering quite similar to the reference one. We may consider 107\n",
    "(namely, c11 + c22 + c33 ) out of the 120 input points as correctly grouped. In particular, \n",
    "all the red and green reference points (the 2nd and the 3rd row) have been properly discovered.\n",
    "\n",
    "Normalized Clustering Accuracy (NCA) \n",
    "NCA is the averaged percentage of correctly classified points in each cluster \n",
    "above the perfectly uniform label distribution.\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def predict(embedding_technique, X, label, benchmark, clustering_method, plot=False):\n",
    "\n",
    "    y_true = benchmark.labels[label] \n",
    "    (k := max(y_true))  # or benchmark.n_clusters[0]\n",
    "    m = max(min(y_true),2)\n",
    "    method = clustering_method.lower()\n",
    "    empty = False\n",
    "\n",
    "    # Define the clustering model\n",
    "    if method == \"genie\":\n",
    "        model = genieclust.Genie(n_clusters=k)  # using default parameters\n",
    "    elif method == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    elif method == \"agglomerative\":\n",
    "        model = AgglomerativeClustering(n_clusters=k)\n",
    "    elif method == \"dbscan\":\n",
    "        model = DBSCAN(eps=0.2, min_samples=m)\n",
    "    elif method == \"meanshift\":\n",
    "        model = MeanShift()\n",
    "    elif method == \"spectral\":\n",
    "        model = SpectralClustering(n_clusters=k, random_state=42)\n",
    "    elif method == \"affinitypropagation\":\n",
    "        model =  AffinityPropagation(random_state=42)\n",
    "    elif method == \"optics\":\n",
    "        model = OPTICS()\n",
    "    elif method == \"gaussianmixture\":\n",
    "        model = GaussianMixture(n_components=k, random_state=42)\n",
    "    elif method == \"hdbscan\":\n",
    "        model = hdbscan.HDBSCAN(min_cluster_size=m)\n",
    "    elif method == \"kmodes\":\n",
    "        model = KModes(n_clusters=k, random_state=42, init=\"Huang\")\n",
    "    elif method == \"birch\":\n",
    "        model = Birch(n_clusters=k)\n",
    "    elif method == \"minibatchkmeans\":\n",
    "        model = MiniBatchKMeans(n_clusters=k, random_state=42)\n",
    "    elif method == \"fcm\":\n",
    "        model = FCM(n_clusters=k)\n",
    "    elif method == \"minisom\":\n",
    "        model = MiniSom(x=10, y=10, input_len=X.shape[1], sigma=1.0, learning_rate=0.5)\n",
    "    elif method == \"kmedoids\":\n",
    "        model = KMedoids(n_clusters=k, random_state=42)\n",
    "    elif method == \"latentdirichletallocation\":\n",
    "        X = np.maximum(X, 0)\n",
    "        model = LatentDirichletAllocation(n_components=k, random_state=42)\n",
    "    elif method == \"spectralcoclustering\":\n",
    "        model =  SpectralCoclustering(n_clusters=k)\n",
    "    elif method == \"bayesiangaussianmixture\":\n",
    "        model = BayesianGaussianMixture(n_components=k)   \n",
    "\n",
    "    print(\"the model: \" +  method + \" has been trained now getting y_pred\") \n",
    "   \n",
    "    # Fit the model and predict the cluster labels\n",
    "    if method == \"gaussianmixture\":  # Gaussian uses predict instead of fit_predict\n",
    "        (y_pred := model.fit(X).predict(X) + 1)\n",
    "    if method == \"fcm\":  # Gaussian uses predict instead of fit_predict\n",
    "        if(model.centers != None):\n",
    "            (y_pred := model.fit(X).predict(X) + 1) \n",
    "        else:\n",
    "            empty = True \n",
    "    elif method == \"minisom\":\n",
    "        model.train(X, 100)\n",
    "        y_pred = np.array([model.winner(x) for x in X]) + 1\n",
    "    elif method == \"latentdirichletallocation\":\n",
    "        model.fit(X)\n",
    "        y_pred = model.transform(X).argmax(axis=1) + 1\n",
    "    elif method == \"spectralcoclustering\":\n",
    "        model.fit(X)\n",
    "        y_pred = y_pred = model.row_labels_ + 1\n",
    "    elif method == \"optics\" or method == \"hdbscan\" or method == \"dbscan\":\n",
    "        y_pred = model.fit_predict(X)\n",
    "        unique_labels = np.unique(y_pred)\n",
    "        if -1 in unique_labels:\n",
    "            y_pred = np.where(y_pred == -1, max(unique_labels) + 1, y_pred)  # Assign noise to a new cluster\n",
    "        y_pred += 1\n",
    "    else:\n",
    "        (y_pred := model.fit_predict(X) + 1)\n",
    "        \n",
    "    # Calculate Clustering Fidelity, NCA (AMI as proxy)\n",
    "    if(empty):\n",
    "        cf = 0\n",
    "        nca = 0\n",
    "    else: \n",
    "        nca = clustbench.get_score(y_true, y_pred)\n",
    "        if len(y_true) != len(y_pred):\n",
    "            y_pred = y_pred[:len(y_true)]\n",
    "        cf = metrics.confusion_matrix(y_true, y_pred)\n",
    "        nca = clustbench.get_score(y_true, y_pred)\n",
    "        r = rand_score(y_true, y_pred)\n",
    "        ar = adjusted_rand_score(y_true, y_pred)\n",
    "        fm = fowlkes_mallows_score(y_true, y_pred)\n",
    "        # afm = \n",
    "        mi = mutual_info_score(y_true, y_pred)\n",
    "        nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "        ami = adjusted_mutual_info_score(y_true, y_pred)\n",
    "        a = accuracy_score(y_true, y_pred)\n",
    "        y_true = np.array(y_true).reshape(-1, 1)  # Convert to 2D array\n",
    "        y_pred = np.array(y_pred).reshape(-1, 1)  # Convert to 2D array\n",
    "        psi = pairwise.cosine_similarity(y_true, y_pred)\n",
    "        \n",
    "\n",
    "\n",
    "    if plot and not empty:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        model.plots.plot_scatter(X, labels=y_true-1, axis=\"equal\", title=\"y_true\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        model.plots.plot_scatter(X, labels=y_pred-1, axis=\"equal\", title=\"y_pred\")\n",
    "        plt.show()\n",
    "\n",
    "    return cf, nca, r, ar, fm, mi, nmi, ami, a, psi\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO unable to figure out the fcm nonetype error and how to handle it would need some help - for now have commented out fcm from the list of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/framework\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd()) # run to check current working directory and update file path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_collections = {\"wut\": [\"x2\"], \"other\": [\"iris\"]}\n",
    "# eval_collections = {\"other\": [\"iris\"]}\n",
    "clustering_methods = [\"genie\"]\n",
    "# clustering_methods = [\"genie\", \"kmeans\", \"agglomerative\", \"dbscan\", \"meanshift\", \"spectral\", \"affinitypropagation\",\"optics\",\"gaussianmixture\", \"hdbscan\", \"kmodes\", \"birch\", \"minibatchkmeans\" ,\"kmedoids\", \"latentdirichletallocation\", \"spectralcoclustering\", \"bayesiangaussianmixture\"]\n",
    "result_csv = \"/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/framework/results/v1_beta_embedding_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Run to set the column names for the csv file\n",
    "\"\"\"\n",
    "import os\n",
    "import csv\n",
    "\n",
    "if os.path.exists(result_csv):\n",
    "    print(\"File already exists\")\n",
    "else:\n",
    "    try:\n",
    "        with open(result_csv, mode='w', newline='') as file: \n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Collection\", \"Dataset\", \"Clustering Method\", \"Label\", \"Embedding\", \"CF\", \"NCA Score\", \"R\", \"AR\", \"FM\", \"MI\", \"NMI\", \"AMI\", \"A\", \"PSI\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error writing to file: \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe create a cache or temporary storage for the embeddings\n",
    "# TODO: parallelize the embedding and clustering process per dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: wut, Dataset: x2\n",
      "Loaded:  120  | Dimension:  2  | Label count:  2\n",
      "Getting embeddings (with caching)...\n",
      "Loading embeddings from cache: embedding_cache/wut_x2_embeddings.pkl\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[12 37  1]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 9 40  1]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[12 37  1]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[11 37  2]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[11 37  2]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[12 37  1]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[12 37  1]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 7 37  6]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[13 36  1]\n",
      " [40  0  0]\n",
      " [10  0 20]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[12 37  1]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[12 37  1]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[12 37  1]\n",
      " [40  0  0]\n",
      " [ 0  0 30]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  6  4  0  0]\n",
      " [ 0  0 22  0  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  0  0 15 16]\n",
      " [ 0  0 11  0  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  3  7  0  0]\n",
      " [ 0  0 22  0  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  0  0 17 14]\n",
      " [ 0  0 11  0  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  6  4  0  0]\n",
      " [ 0  0 22  0  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  0  0 15 16]\n",
      " [ 0  0 11  0  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  5  0  4  1]\n",
      " [ 0  0 17  5  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  0  0  0 31]\n",
      " [ 0  0  6  5  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  5  4  1  0]\n",
      " [ 0  0 22  0  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  0  0 15 16]\n",
      " [ 0  0 11  0  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  1  5  4  0]\n",
      " [ 0  0  0 22  0]\n",
      " [ 0 32 14  0  0]\n",
      " [ 0  0  0  0 31]\n",
      " [ 0  0  0 11  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  6  4  0  0]\n",
      " [ 0  0 22  0  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  0  0 15 16]\n",
      " [ 0  0 11  0  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  1  0  5  4]\n",
      " [ 0  0  0  0 22]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  0  0 31  0]\n",
      " [ 0  0 11  0  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  6  0  4  0]\n",
      " [ 0  1  0 21  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0 10  0  0 21]\n",
      " [ 0  0 11  0  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  6  4  0  0]\n",
      " [ 0  0 22  0  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  0  0 15 16]\n",
      " [ 0  0 11  0  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  5  0  4  1]\n",
      " [ 0  0 17  5  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  1  0  0 30]\n",
      " [ 0  0  6  5  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[ 0  6  4  0  0]\n",
      " [ 0  0 22  0  0]\n",
      " [ 0 46  0  0  0]\n",
      " [ 0  0  0 15 16]\n",
      " [ 0  0 11  0  0]]\n",
      "Collection: other, Dataset: iris\n",
      "Loaded:  150  | Dimension:  4  | Label count:  1\n",
      "Getting embeddings (with caching)...\n",
      "Loading embeddings from cache: embedding_cache/other_iris_embeddings.pkl\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0  3 47]\n",
      " [ 0 35 15]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0  3 47]\n",
      " [ 0 36 14]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0  3 47]\n",
      " [ 0 35 15]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0 11 39]\n",
      " [ 0 17 33]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0 23 27]\n",
      " [ 0 49  1]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0 15 35]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0 49  1]\n",
      " [ 0 15 35]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0 15 35]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0 25 25]\n",
      " [ 0 49  1]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0 18 32]\n",
      " [ 0 49  1]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0 27 23]\n",
      " [ 0 50  0]]\n",
      "the model: genie has been trained now getting y_pred\n",
      "[[50  0  0]\n",
      " [ 0 45  5]\n",
      " [ 0  1 49]]\n"
     ]
    }
   ],
   "source": [
    "with open(result_csv, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for collection, datasets in eval_collections.items():\n",
    "        for dataset in datasets:\n",
    "            print(f\"Collection: {collection}, Dataset: {dataset}\")\n",
    "            X, benchmark, X_embedded_dict = load_data(collection, dataset)\n",
    "            \n",
    "            for label in range(0, len(benchmark.labels)):\n",
    "                for embedding_technique, embedded_data in X_embedded_dict.items():\n",
    "                    for clustering_method in clustering_methods:\n",
    "                        cf, nca_score, r, ar, fm, mi, nmi, ami, a, psi = predict(\n",
    "                            embedding_technique, \n",
    "                            embedded_data, \n",
    "                            label, \n",
    "                            benchmark, \n",
    "                            clustering_method\n",
    "                        )\n",
    "                        print(cf)\n",
    "                        cf_str = \", \".join(map(str, cf.flatten())) if hasattr(cf, \"flatten\") else \", \".join(map(str, cf))\n",
    "\n",
    "                        writer.writerow([\n",
    "                            collection,          # e.g. \"wut\"\n",
    "                            dataset,             # e.g. \"x2\"\n",
    "                            clustering_method,   # e.g. \"genie\"\n",
    "                            label,               # which label set index (0, 1, ...)\n",
    "                            embedding_technique, # e.g. \"PCA\", \"t-SNE\", ...\n",
    "                            f\"cf: {cf} \", # confusion matrix\n",
    "                            f\"nca: {nca_score} \", # normalized clustering accuracy\n",
    "                            f\"r: {r} \",  # rand index\n",
    "                            f\"ar: {ar} \",  # adjusted rand index\n",
    "                            f\"fm: {fm} \",  # fowlkes-mallows index\n",
    "                            f\"mi: {mi} \",  # mutual information\n",
    "                            f\"nmi: {nmi} \",  # normalized mutual information\n",
    "                            f\"ami: {ami} \",  # adjusted mutual information\n",
    "                            f\"a: {a} \",  # accuracy score\n",
    "                            f\"psi: {psi} \",  # pairwise cosine similairty index\n",
    "                            \n",
    "                        ])\n",
    "# AR (Adjusted Rand Index): Measures the similarity between two data clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings, adjusted for chance.\n",
    "\n",
    "# R (Rand Index): Similar to AR but not adjusted for chance. It measures the percentage of correct decisions made by the clustering algorithm.\n",
    "\n",
    "# FM (Fowlkes-Mallows Index): Measures the similarity between two clusterings by considering the geometric mean of the precision and recall.\n",
    "\n",
    "# AFM (Adjusted Fowlkes-Mallows Index): An adjusted version of the Fowlkes-Mallows Index that accounts for chance.\n",
    "\n",
    "# MI (Mutual Information): Measures the amount of information obtained about one clustering from the other clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Collection Dataset Clustering Method Label       Embedding       NCA\n",
      "0        wut      x2             genie     0            Base  0.870000\n",
      "1        wut      x2             genie     0       KernelPCA  0.900000\n",
      "2        wut      x2             genie     1            Base  0.505376\n",
      "3        wut      x2             genie     1       KernelPCA  0.516129\n",
      "4        wut      x2             genie     1  FactorAnalysis  0.742424\n",
      "5        wut      x2             genie     1            UMAP  0.565217\n",
      "6        wut      x2             genie     1          Isomap  1.000000\n",
      "7        wut      x2             genie     1             LLE  0.877322\n",
      "8        wut      x2             genie     1      GaussianRP  0.731672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/t8jvtbb97_jf_k38b6ptvj480000gn/T/ipykernel_34401/1558977567.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_and_compare_csv(file_path):\n",
    "    # Define column names based on the CSV structure\n",
    "    col_names = [\n",
    "        \"Collection\", \"Dataset\", \"Clustering Method\", \"Label\", \"Embedding\", \n",
    "        \"CF\", \"NCA\", \"r\", \"ar\", \"fm\", \"mi\", \"nmi\", \"ami\", \"a\", \"psi\"\n",
    "    ]\n",
    "    \n",
    "    # Read CSV without a header, assigning our own column names\n",
    "    df = pd.read_csv(file_path, header=None, names=col_names)\n",
    "    \n",
    "    # Remove extra whitespace from all string cells\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    \n",
    "    # Function to clean a numeric field with a prefix\n",
    "    def clean_numeric(value, prefix):\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(prefix, \"\").strip()\n",
    "        try:\n",
    "            return float(value)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # Clean the NCA column by removing the \"nca:\" prefix and converting to float\n",
    "    df['NCA'] = df['NCA'].apply(lambda x: clean_numeric(x, 'nca:'))\n",
    "    \n",
    "    # Group by the specified columns\n",
    "    grouped = df.groupby([\"Collection\", \"Dataset\", \"Clustering Method\", \"Label\"])\n",
    "    \n",
    "    filtered_rows = []\n",
    "    \n",
    "    # Iterate over each group\n",
    "    for name, group in grouped:\n",
    "        # Find the \"Base\" row in the Embedding column\n",
    "        base_row = group[group['Embedding'] == 'Base']\n",
    "        if not base_row.empty:\n",
    "            base_value = base_row.iloc[0]['NCA']\n",
    "            base_row_list = base_row.iloc[0].tolist()\n",
    "            base_added = False\n",
    "            \n",
    "            # Compare each row's NCA value to the base_value\n",
    "            for index, row in group.iterrows():\n",
    "                if row['Embedding'] != 'Base' and row['NCA'] > base_value:\n",
    "                    if not base_added:\n",
    "                        filtered_rows.append(base_row_list)\n",
    "                        base_added = True\n",
    "                    filtered_rows.append(row.tolist())\n",
    "    \n",
    "    # Create a new DataFrame from the filtered rows and remove duplicates\n",
    "    filtered_df = pd.DataFrame(filtered_rows, columns=df.columns)\n",
    "    filtered_df = filtered_df.drop_duplicates()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Example usage\n",
    "file_path = \"/Users/cajoshuapark/Dev/research/embedding_based_clustering_research/framework/results/v1_beta_embedding_test.csv\"\n",
    "filtered_df = filter_and_compare_csv(file_path)\n",
    "\n",
    "# Select only the desired columns for display\n",
    "display_columns = [\"Collection\", \"Dataset\", \"Clustering Method\", \"Label\", \"Embedding\", \"NCA\"]\n",
    "print(filtered_df[display_columns])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
